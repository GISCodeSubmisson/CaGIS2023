{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from urllib.request import urlretrieve\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "#from torch import nn\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import dgl\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "import fiona\n",
    "import statistics\n",
    "import shapely\n",
    "from shapely.geometry import *\n",
    "from shapely import affinity\n",
    "\n",
    "# Import tensorboard logger from PyTorch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Load tensorboard extension for Jupyter Notebook, only need to start TB in the notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_coords(coordlist, length):\n",
    "    for i in range(length-len(coordlist)):\n",
    "        dist_dict = {}\n",
    "        for i in range(len(coordlist)-1):\n",
    "            dist_dict[i]=LineString((coordlist[i], coordlist[i+1])).length\n",
    "        sort = dict(sorted(dist_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        line_new = [coordlist[i] for i in range(len(coordlist))]\n",
    "        line_new.insert(next(iter(sort))+1, LineString((coordlist[next(iter(sort))], coordlist[next(iter(sort))+1])).interpolate(0.5, normalized = True).coords[0])\n",
    "        coordlist = line_new\n",
    "    return coordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from Yan et al. 2021\n",
    "\n",
    "def normalize_shape_geometry(buildings, scaling = 'x', scale_type = 'minmax'):\n",
    "    df_use = copy.deepcopy(buildings)\n",
    "    \n",
    "    # Verschiebung in den Mittelpunkt auf Basis Centroid\n",
    "   \n",
    "    df_use['mu_x'] = pd.Series([geo.x for geo in df_use['centroid_norm']])\n",
    "    df_use['mu_y'] = pd.Series([geo.y for geo in df_use['centroid_norm']])\n",
    "    df_use['geometry'] = pd.Series(\n",
    "        [affinity.translate(geo, -mx, -my) for mx, my, geo in df_use[['mu_x', 'mu_y', 'geometry']].values])\n",
    "    \n",
    "    if scale_type == 'minmax':\n",
    "        # Maximalster & Minimalster Wert für x\n",
    "        df_use['x_max'] = pd.Series([max(geo.exterior.xy[0]) for geo in df_use['geometry']])\n",
    "        df_use['x_min'] = pd.Series([min(geo.exterior.xy[0]) for geo in df_use['geometry']])\n",
    "        df_use['scale_x'] = (df_use['x_max'] - df_use['x_min'])\n",
    "    \n",
    "        # Maximalster & Minimalster Wert für y\n",
    "        df_use['y_max'] = pd.Series([max(geo.exterior.xy[1]) for geo in df_use['geometry']])\n",
    "        df_use['y_min'] = pd.Series([min(geo.exterior.xy[1]) for geo in df_use['geometry']])\n",
    "        df_use['scale_y'] = (df_use['y_max'] - df_use['y_min'])\n",
    "        \n",
    "        df_use.drop(['x_max', 'x_min', 'y_max', 'y_min'], axis=1, inplace=True)\n",
    "    \n",
    "    elif scale_type == 'stdev':\n",
    "        df_use['bds']= pd.Series([list(geo.bounds) for geo in df_use['geometry']])\n",
    "        df_use['dist']= pd.Series([centroid.distance(Point(bbox[0], bbox[1])) for centroid, bbox in df_use[['centroid_norm', 'bds']].values])\n",
    "        df_use['scale_x'] = df_use['dist'].std()\n",
    "        df_use['scale_y'] = df_use['dist'].std()\n",
    "        \n",
    "        df_use.drop(['bds', 'dist'], axis=1, inplace=True)\n",
    "\n",
    "    # Geometrie skaliert auf Werte von -1~1 \n",
    "    if scaling == 'xy': # Achsen individuell skaliert\n",
    "        df_use['geometry'] = pd.Series(\n",
    "            [affinity.scale(geo, 1 / del_x, 1 / del_y, origin='centroid') for del_x, del_y, geo in\n",
    "             df_use[['scale_x', 'scale_y', 'geometry']].values])\n",
    "    \n",
    "    elif scaling == 'x': # auf Basis x-Achse skaliert \n",
    "        df_use['geometry'] = pd.Series([affinity.scale(geo, 1 / del_x, 1 / del_x, origin='centroid') for del_x, geo in\n",
    "                                        df_use[['scale_x', 'geometry']].values])\n",
    "        \n",
    "    elif scaling == 'y': # auf Basis x-Achse skaliert \n",
    "        df_use['geometry'] = pd.Series([affinity.scale(geo, 1 / del_y, 1 / del_y, origin='centroid') for del_y, geo in\n",
    "                                        df_use[['scale_y', 'geometry']].values])\n",
    "    \n",
    "    df_use['centroid'] = Point(0,0)\n",
    "    \n",
    "    df_use.drop(['mu_x', 'mu_y'], axis=1, inplace=True)\n",
    "\n",
    "    return df_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative normalization method\n",
    "\n",
    "def normalize_shape_map(buildings, scaling = 'x', scale_type = 'minmax'):\n",
    "    df_use = copy.deepcopy(buildings)\n",
    "    \n",
    "    df_use['min_x'] = df_use.unary_union.bounds[0]\n",
    "    df_use['min_y'] = df_use.unary_union.bounds[1]\n",
    "    df_use['max_x'] = df_use.unary_union.bounds[2]\n",
    "    df_use['max_y'] = df_use.unary_union.bounds[3]\n",
    "    \n",
    "    df_use['geometry'] = pd.Series(\n",
    "        [affinity.translate(geo, -mx, -my) for mx, my, geo in df_use[['min_x', 'min_y', 'geometry']].values])\n",
    "    \n",
    "    \n",
    "    if scale_type == 'minmax':\n",
    "        df_use['bbox'] = df_use.unary_union.envelope\n",
    "\n",
    "        p1 = Point(df_use.iloc[0]['bbox'].exterior.coords[0])\n",
    "        p2 = Point(df_use.iloc[0]['bbox'].exterior.coords[1])\n",
    "        p3 = Point(df_use.iloc[0]['bbox'].exterior.coords[2])\n",
    "        l1 = LineString([p1,p2])\n",
    "        l2 = LineString([p2,p3])\n",
    "        max_length = l1.length if l1.length > l2.length else l2.length\n",
    "        df_use['scale_max'] = max_length/2\n",
    "        \n",
    "        df_use.drop(['max_x', 'min_x', 'max_y', 'min_y'], axis=1, inplace=True)\n",
    "        \n",
    "    elif scale_type == 'stdev':\n",
    "        pass\n",
    "\n",
    "        # Geometrie skaliert auf Werte von -1~1 \n",
    "    if scaling == 'xy': # Achsen individuell skaliert\n",
    "        df_use['x_scale'] = l1.length/2\n",
    "        df_use['y_scale'] = l2.length/2\n",
    "        \n",
    "        df_use['geometry'] = pd.Series([affinity.scale(geo, xfact = 1/x_scale, yfact = 1/y_scale, origin=(0,0)) \n",
    "                                            for x_scale, y_scale, geo in df_use[['x_scale', 'x_scale', 'geometry']].values])\n",
    "        df_use['geometry'] = pd.Series([affinity.translate(geo, -1, -1) for geo in df_use['geometry'].values])\n",
    "        \n",
    "        df_use.drop(['x_scale', 'y_scale'], axis=1, inplace=True)\n",
    "    \n",
    "    elif scaling == 'x': # auf Basis x-Achse skaliert \n",
    "    \n",
    "        df_use['geometry'] = pd.Series([affinity.scale(geo, xfact = 1/max_length, yfact = 1/max_length, origin=(0,0)) \n",
    "                                            for max_length, geo in df_use[['scale_max','geometry']].values])\n",
    "        df_use['geometry'] = pd.Series([affinity.translate(geo, -1, -1) for geo in df_use['geometry'].values])\n",
    "        \n",
    "    elif scaling == 'y': # auf Basis y-Achse skaliert \n",
    "        df_use['geometry'] = pd.Series([affinity.scale(geo, xfact = 1/max_length, yfact = 1/max_length, origin=(0,0)) \n",
    "                                            for max_length, geo in df_use[['scale_max','geometry']].values])\n",
    "        df_use['geometry'] = pd.Series([affinity.translate(geo, -1, -1) for geo in df_use['geometry'].values])\n",
    "    \n",
    "    df_use['centroid'] = Point(0,0)\n",
    "\n",
    "    return df_use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_geometries(buildings, shape_dict, max_len, centroid_type = 'geometry', label_col = 'name', mode = 'iterativ'):\n",
    "    '''\n",
    "        buildings: gdf of buildings\n",
    "        shape_dict: dict for class names\n",
    "        max_len: length of geometries (=number of points). Padding if 0. Longer geometries are not encoded\n",
    "     '''\n",
    "    \n",
    "    df_buildings = copy.deepcopy(buildings)\n",
    "\n",
    "    \n",
    "    df_buildings['shape_char'] = pd.Series([shape_dict[n] for n in df_buildings[label_col]])\n",
    "    df_buildings['centroid'] = gpd.GeoSeries([geo.centroid for geo in df_buildings['geometry']])\n",
    "    df_buildings['centroid_orig'] = gpd.GeoSeries([geo.centroid for geo in df_buildings['geometry']])\n",
    "    \n",
    "    if centroid_type == 'geometry':\n",
    "        df_buildings['centroid_norm'] = pd.Series([geo.centroid for geo in df_buildings['geometry']])\n",
    "    elif centroid_type == 'map':\n",
    "        df_buildings['centroid_norm'] = df_buildings.unary_union.centroid\n",
    "    \n",
    "    df_buildings['orig_len'] = pd.Series([len(geo.exterior.coords) for geo in df_buildings['geometry']])\n",
    "    df_buildings['encod_len'] = pd.Series([int(len(geo.exterior.coords)) for geo in df_buildings['geometry']])\n",
    "    if max_len != 0:\n",
    "        \n",
    "        if mode == 'iterativ':\n",
    "            df_buildings['geometry'] = pd.Series([Polygon(extend_coords(list(geo.exterior.coords), max_len)) \n",
    "                                                  for geo in df_buildings['geometry']])\n",
    "        elif mode == 'interpolate':\n",
    "            df_buildings['geometry'] = pd.Series([Polygon([Point(LineString(geo.exterior.coords).interpolate(i/(max_len-1), normalized=True)) \n",
    "                                                           for i in range(max_len-1)]) for geo in df_buildings['geometry']])\n",
    "        elif mode == 'padding':\n",
    "            pass\n",
    "        \n",
    "        df_buildings['encod_len'] = max_len\n",
    "        \n",
    "        long_geoms = df_buildings[ df_buildings['orig_len'] > max_len].index\n",
    "        df_buildings.drop(long_geoms, inplace = True)\n",
    "        #df_buildings.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_geometries(buildings, sampling = 'none', samples = 1,\n",
    "                id_name = 'OBJECTID'):\n",
    "    '''\n",
    "        buildings: gdf of buildings\n",
    "        shape_dict: dict for class names\n",
    "        max_len: length of geometries (=number of points). Padding if 0. Longer geometries are not encoded\n",
    "        sampling: 'none' if no sampling, 'rotate' if regular rotating, 'randomrotate' if random rotating, default = 'None'\n",
    "        samples: number of objects per building, default = 1\n",
    "        id_name: id attribute in buildings properties\n",
    "     '''\n",
    "    \n",
    "    assert samples > 0, \"samples must be > 0\"\n",
    "    \n",
    "    if sampling == 'rotate':\n",
    "        angles = list(range(360//samples, 360, 360//samples))\n",
    "    elif sampling == 'randomrotate':\n",
    "        angles = random.sample(range(360), samples-1)\n",
    "    else:\n",
    "        angles = []\n",
    "    \n",
    "    df_buildings = copy.deepcopy(buildings)\n",
    "    \n",
    "    df_buildings[id_name] = df_buildings[id_name]*1000\n",
    "    \n",
    "    for index, row in buildings.iterrows():\n",
    "        #print(index)\n",
    "        #print(row['centroid'])\n",
    "        id_ = row[id_name]*1000\n",
    "        geom = row['geometry']\n",
    "        center = row['centroid']\n",
    "        if row['status'] == 'train':\n",
    "            for i in angles:\n",
    "                row[id_name] = id_ + i\n",
    "                row['geometry'] = affinity.rotate(geom, i, origin=center)\n",
    "                df_buildings = df_buildings.append(row)\n",
    "    \n",
    "    df_buildings.sort_values([id_name], axis = 0, inplace=True)\n",
    "    df_buildings.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    return df_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(a, b, c):\n",
    "    if LineString([a, b]).length > LineString([b, c]).length:\n",
    "        lon_len = LineString([a,b])\n",
    "    else: \n",
    "        lon_len = LineString([b,c])\n",
    "    \n",
    "    p1 = np.array(lon_len.coords[1])\n",
    "    p2 = np.array(lon_len.coords[0])\n",
    "\n",
    "    # checks orientation of p vector & selects appropriate y_axis_vector\n",
    "    if (p2[1] - p1[1]) < 0:\n",
    "        y_axis_vector = np.array([0, -1])\n",
    "    else:\n",
    "        y_axis_vector = np.array([0, 1])\n",
    "\n",
    "    if (p2[0] - p1[0]) < 0 and (p2[1] - p1[1]) :\n",
    "        y_axis_vector = np.array([0, 1])\n",
    "\n",
    "    p_unit_vector = (p2 - p1) / np.linalg.norm(p2-p1)\n",
    "    return np.arccos(np.dot(p_unit_vector, y_axis_vector)) * 180 /math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_to_y_axis(geom):\n",
    "    mbr = geom.minimum_rotated_rectangle\n",
    "    a, b, c = mbr.exterior.coords[0:3]\n",
    "    rotation_angle = get_angle(a, b, c)\n",
    "    mbr1 = affinity.rotate(mbr, rotation_angle)\n",
    "    a1, b1, c1 = mbr1.exterior.coords[0:3]\n",
    "    if get_angle(a1, b1, c1) > rotation_angle:\n",
    "        return -rotation_angle\n",
    "    else:\n",
    "        return rotation_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_geom_to_y_axis(df, which_data = 'all'):\n",
    "    df['rotate_angle'] = pd.Series([rotate_to_y_axis(geo) for geo in df['geometry']])\n",
    "    df['orig_geometry'] = df['geometry']\n",
    "    if which_data == 'train':\n",
    "        df['geometry'] = gpd.GeoSeries([affinity.rotate(geo, angle) if (status == 'train') else geo for geo, angle, status in df[['orig_geometry', 'rotate_angle', 'status']].values])\n",
    "    elif which_data == 'test':\n",
    "        df['geometry'] = gpd.GeoSeries([affinity.rotate(geo, angle) if (status == 'test') else geo for geo, angle, status in df[['orig_geometry', 'rotate_angle', 'status']].values])\n",
    "    else:\n",
    "        df['geometry'] = gpd.GeoSeries([affinity.rotate(geo, angle) for geo, angle in df[['orig_geometry', 'rotate_angle']].values])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    return dgl.batch(graphs), torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderedSeqs_Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, input_geoms, target_labels, padding = False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_geoms - input geometries\n",
    "            target_labels - target labels\n",
    "            padding - if padding when preprocessing\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_geoms = input_geoms\n",
    "        self.target_labels = target_labels\n",
    "        #self.seq_len = max_sequence_length\n",
    "        self.generate_dataset()\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        \n",
    "        zipped = zip(self.input_geoms, self.target_labels)\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        for geoms, target in zipped:\n",
    "\n",
    "            data.append(torch.tensor(geoms))\n",
    "            labels.append(torch.tensor(target))\n",
    "\n",
    "        self.data = data\n",
    "        self.label = labels\n",
    "        self.size = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_geometry = self.data[idx]\n",
    "        data_label = self.label[idx]\n",
    "        return data_geometry, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, input_encoding, target_labels, seq_lens, padding = False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_encoding: list of input encodings - input geometries\n",
    "            target_labels: list of target labels\n",
    "            seq_lens: list of sequence lengths of encodings\n",
    "            padding - if padding when preprocessing\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_encoding = input_encoding\n",
    "        self.seq_lens = seq_lens\n",
    "        self.target_labels = target_labels\n",
    "        self.generate_dataset()\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        \n",
    "        all_data_range=0\n",
    "        graph = []\n",
    "        for i in range(len(self.input_encoding)):\n",
    "            point = self.seq_lens[i]\n",
    "            uu = []\n",
    "            vv = []\n",
    "            for j in range(point):\n",
    "                uu.append(j)\n",
    "                if j + 1 in range(point):\n",
    "                    vv.append(j+1)\n",
    "                else:\n",
    "                    vv.append(0)\n",
    "            u = np.concatenate([uu, vv])\n",
    "            v = np.concatenate([vv, uu])\n",
    "            g = dgl.graph((torch.tensor(uu), torch.tensor(vv)))\n",
    "            g = dgl.to_bidirected(g)\n",
    "            g.edges()\n",
    "\n",
    "            g.ndata['x'] = torch.tensor(self.input_encoding[i], dtype=torch.float64)\n",
    "            graph.append(g)\n",
    "        \n",
    "        data = []\n",
    "        for i in range(len(graph)):\n",
    "            temp = (graph[i], self.target_labels[i])\n",
    "            data.append(temp)\n",
    "\n",
    "        self.data = data\n",
    "        self.size = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_2(building, centroid, scale):\n",
    "    coord_list = list(building.exterior.coords)\n",
    "    centroid = centroid.coords[0]\n",
    "    tensor_rep_orig = []\n",
    "    for coordinate in coord_list[:-1]:\n",
    "        tensor_rep_orig.append([(centroid[0]-coordinate[0])/scale, (centroid[1]-coordinate[1])/scale])\n",
    "    tensor_rep_orig.append([(centroid[0]-coord_list[0][0])/scale, (centroid[1]-coord_list[0][1])/scale])\n",
    "    return tensor_rep_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_3(building, centroid, scale, intermediate = True):\n",
    "    coord_list = list(building.exterior.coords)\n",
    "    centroid = centroid.coords[0]\n",
    "    tensor_rep_orig = []\n",
    "    for coordinate in coord_list[:-1]:\n",
    "        tensor_rep_orig.append([(centroid[0]-coordinate[0])/scale, (centroid[1]-coordinate[1])/scale, int(intermediate)])\n",
    "    tensor_rep_orig.append([(centroid[0]-coord_list[0][0])/scale, (centroid[1]-coord_list[0][1])/scale, int(not intermediate)])\n",
    "    return tensor_rep_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_4(building, centroid, scale, intermediate = True):\n",
    "    coord_list = list(building.exterior.coords)\n",
    "    centroid = centroid.coords[0]\n",
    "    tensor_rep_orig = []\n",
    "    for coordinate in coord_list[:-1]:\n",
    "        tensor_rep_orig.append([(centroid[0]-coordinate[0])/scale, (centroid[1]-coordinate[1])/scale, int(intermediate), int(not intermediate)])\n",
    "    tensor_rep_orig.append([(centroid[0]-coord_list[0][0])/scale, (centroid[1]-coord_list[0][1])/scale, int(not intermediate), int(intermediate)])\n",
    "    return tensor_rep_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_5(building, centroid, scale, intermediate = True):\n",
    "    coord_list = list(building.exterior.coords)\n",
    "    centroid = centroid.coords[0]\n",
    "    tensor_rep_orig = []\n",
    "    for coordinate in coord_list[:-1]:\n",
    "        tensor_rep_orig.append([(centroid[0]-coordinate[0])/scale, (centroid[1]-coordinate[1])/scale, int(intermediate), int(not intermediate), int(not intermediate)])\n",
    "    tensor_rep_orig.append([(centroid[0]-coord_list[0][0])/scale, (centroid[1]-coord_list[0][1])/scale, int(not intermediate), int(not intermediate), int(intermediate)])\n",
    "    return tensor_rep_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from Yan et al. 2021\n",
    "\n",
    "def encoding_graph_features(uid, geo, local_features, regional_features, global_features_polygon, \n",
    "                            klst = [2, 4], normalize = 'minmax'):    \n",
    "    \n",
    "    \n",
    "    cols = []\n",
    "    if 'triangle_area' in local_features:\n",
    "        cols.append('s_abc')\n",
    "    if 'len_c' in local_features:\n",
    "        cols.append('l_ab')\n",
    "    if 'len_b' in local_features:\n",
    "        cols.append('l_ac')\n",
    "    if 'angle_a' in local_features:\n",
    "        cols.append('angle_bac')\n",
    "    if 'len_a' in local_features:\n",
    "        cols.append('l_bc')\n",
    "    if 'height' in local_features:\n",
    "        cols.append('height_a')    \n",
    " \n",
    "    if 'reg_triangle_area' in regional_features:\n",
    "        cols.append('s_obc')\n",
    "    if 'reg_len_c' in regional_features:\n",
    "        cols.append('l_ob')\n",
    "    if 'reg_len_b' in regional_features:\n",
    "        cols.append('l_oc')\n",
    "    if 'reg_angle_o' in regional_features:\n",
    "        cols.append('angle_boc')\n",
    "    if 'reg_len_o' in regional_features:\n",
    "        cols.append('l_bc')\n",
    "    if 'reg_height' in regional_features:\n",
    "        cols.append('height_o')\n",
    "    if 'semiperimeter' in regional_features:\n",
    "        cols.append('c_obc')    \n",
    "    if 'radius' in regional_features:\n",
    "        cols.append('r_obc') \n",
    "        \n",
    "        \n",
    "        \n",
    "    pDic = {}\n",
    "    \n",
    "    geom_points = list(geo.exterior.coords)#[:-1]\n",
    "    geom_points_gdf = gpd.GeoDataFrame(geom_points)\n",
    "    geom_points_gdf.columns = ['a_x', 'a_y']\n",
    "    geom_points_gdf['o_x'] = 0\n",
    "    geom_points_gdf['o_y'] = 0\n",
    "    geom_points_gdf['UID'] = uid\n",
    "    geom_points_gdf['PID'] = pd.Series([i for i in range(len(geom_points_gdf))])\n",
    "    geom_points_gdf['OID_UID'] = pd.Series([uid*1000+ pid for uid, pid in geom_points_gdf[['UID', 'PID']].values])\n",
    "    \n",
    "    #geom_points_gdf = gpd.GeoDataFrame()\n",
    "    for k in klst:\n",
    "        dft = get_single_features_final_new(uid, geo, k)\n",
    "        dft.columns = ['k{0}_{1}'.format(k, x) if x in cols else x for x in dft.columns]\n",
    "        pDic[k] = dft\n",
    "    columns_all = []\n",
    "    for k in klst:\n",
    "        newcols = ['k{0}_{1}'.format(k, x) for x in cols if 'k{0}_{1}'.format(k, x) in pDic[k].columns]\n",
    "        if 'mean_dist_o' in global_features_polygon:\n",
    "            if k == klst[-1]:\n",
    "                newcols.append('l_oa')\n",
    "        geom_points_gdf = pd.merge(geom_points_gdf, pDic[k][['OID_UID'] + newcols], how='left', on='OID_UID')\n",
    "        for n in newcols:\n",
    "            columns_all.append(n)\n",
    "    #df_detail=df_detail.dropna(axis=1)\n",
    "    \n",
    "    \n",
    "    # MBR\n",
    "    df_mbr = gpd.GeoDataFrame()\n",
    "    df_mbr['geometry'] = pd.Series(geo.minimum_rotated_rectangle) # min-rotated rectangle\n",
    "    df_mbr['xy'] = pd.Series([list(geo.exterior.coords) for geo in df_mbr['geometry']]) # coords of mrr\n",
    "    \n",
    "    # Koordinaten für Winkel & Seitenlängen\n",
    "    df_mbr['x0'] = pd.Series([xy[0][0] for xy in df_mbr['xy']])\n",
    "    df_mbr['x1'] = pd.Series([xy[1][0] for xy in df_mbr['xy']])\n",
    "    df_mbr['x2'] = pd.Series([xy[2][0] for xy in df_mbr['xy']])\n",
    "\n",
    "    df_mbr['y0'] = pd.Series([xy[0][1] for xy in df_mbr['xy']])\n",
    "    df_mbr['y1'] = pd.Series([xy[1][1] for xy in df_mbr['xy']])\n",
    "    df_mbr['y2'] = pd.Series([xy[2][1] for xy in df_mbr['xy']])\n",
    "    \n",
    "    # Berechnungen Winkel & Seitenlängen\n",
    "    df_mbr['l1'] = pd.Series(\n",
    "        [cal_euclidean([x0, y0], [x1, y1]) for x0, y0, x1, y1 in df_mbr[['x0', 'y0', 'x1', 'y1']].values])\n",
    "    df_mbr['l2'] = pd.Series(\n",
    "        [cal_euclidean([x0, y0], [x1, y1]) for x0, y0, x1, y1 in df_mbr[['x1', 'y1', 'x2', 'y2']].values])\n",
    "\n",
    "    df_mbr['a1'] = pd.Series(\n",
    "        [cal_arc([x0, y0], [x1, y1], True) for x0, y0, x1, y1 in df_mbr[['x0', 'y0', 'x1', 'y1']].values])\n",
    "    df_mbr['a2'] = pd.Series(\n",
    "        [cal_arc([x0, y0], [x1, y1], True) for x0, y0, x1, y1 in df_mbr[['x1', 'y1', 'x2', 'y2']].values])\n",
    "    #\n",
    "    df_mbr['longer'] = df_mbr['l1'] >= df_mbr['l2']\n",
    "    #\n",
    "    \n",
    "    \n",
    "    geom_points_gdf['lon_len'] = pd.Series([l1 if longer else l2 for l1, l2, longer in df_mbr[['l1', 'l2', 'longer']].values]).iloc[0]\n",
    "    geom_points_gdf['short_len'] = pd.Series([l2 if longer else l1 for l1, l2, longer in df_mbr[['l1', 'l2', 'longer']].values]).iloc[0]\n",
    "    \n",
    "    # weitere Attribute\n",
    "    \n",
    "    geom_points_gdf['Area'] = geo.area\n",
    "    geom_points_gdf['Perimeter'] = geo.exterior.length\n",
    "    geom_points_gdf['Area_convex'] = geo.convex_hull.area\n",
    "    \n",
    "    if 'area' in global_features_polygon:\n",
    "        geom_points_gdf['A'] = geom_points_gdf['Area']\n",
    "    if 'mabr' in global_features_polygon:\n",
    "        geom_points_gdf['MABR'] = geo.minimum_rotated_rectangle.area\n",
    "    if 'elongation' in global_features_polygon:\n",
    "        geom_points_gdf['Elongation'] = pd.Series([short_len / lon_len for short_len, lon_len in geom_points_gdf[['short_len','lon_len']].values])\n",
    "    if 'circularity' in global_features_polygon:\n",
    "        geom_points_gdf['Circularity'] = 4 * np.pi * geom_points_gdf['Area'] / geom_points_gdf['Perimeter'] / geom_points_gdf['Perimeter']\n",
    "    if 'rectangularity' in global_features_polygon:\n",
    "        geom_points_gdf['Rectangularity'] = pd.Series([area / geo.minimum_rotated_rectangle.area for area in geom_points_gdf['Area'].values])\n",
    "    if 'squareness' in global_features_polygon:\n",
    "        geom_points_gdf['Squareness'] = pd.Series([(4*np.sqrt(area) / perimeter)**2 for area, perimeter in geom_points_gdf[['Area', 'Perimeter']].values])    \n",
    "    if 'convexity' in global_features_polygon:\n",
    "        geom_points_gdf['Convexity'] = geom_points_gdf['Area'] / geom_points_gdf['Area_convex']\n",
    "    if 'fractality' in global_features_polygon:\n",
    "        #geom_points_gdf['Fractality'] = 0 # not implemented yet\n",
    "        pass\n",
    "    if 'orientation' in global_features_polygon:\n",
    "        geom_points_gdf['Orientation'] = rotate_to_y_axis(geo)\n",
    "          \n",
    "    #\n",
    "    if 'mean_dist_o' in global_features_polygon:\n",
    "        dfg = geom_points_gdf[['UID', 'l_oa']].groupby(['UID'], as_index=False)['l_oa'].agg({'MeanRadius': 'mean'})\n",
    "        #\n",
    "        #df_features = pd.merge(df_features, dft, how='left', on='OBJECTID')\n",
    "        geom_points_gdf = pd.merge(geom_points_gdf, dfg, how='left', on='UID')\n",
    "        geom_points_gdf.drop(['l_oa'], axis=1, inplace=True)    \n",
    "    \n",
    "    geom_points_gdf.drop(['a_x', 'a_y', 'o_x', 'o_y', 'UID', 'PID', 'OID_UID',\n",
    "                         'lon_len', 'short_len', 'Area', 'Perimeter', 'Area_convex'], axis=1, inplace=True)\n",
    "\n",
    "    if normalize != 'none':\n",
    "        geom_points_gdf = get_normalize_features_final(geom_points_gdf, normalize)\n",
    "    \n",
    "    return np.array(geom_points_gdf.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding using point coordinates and angle at each point\n",
    "\n",
    "def encoding_graph2_features(uid, geo, normalize = 'minmax'):\n",
    "    \n",
    "    k = 1\n",
    "    geom_points = list(geo.exterior.coords)#[:-1]\n",
    "    geom_points_gdf = gpd.GeoDataFrame(geom_points)\n",
    "    geom_points_gdf.columns = ['a_x', 'a_y']\n",
    "    geom_points_gdf['UID'] = uid\n",
    "    geom_points_gdf['PID'] = pd.Series([i for i in range(len(geom_points_gdf))])\n",
    "    geom_points_gdf['OID_UID'] = pd.Series([uid*1000+ pid for uid, pid in geom_points_gdf[['UID', 'PID']].values])\n",
    "    geom_points_gdf['b_key'] = pd.Series(\n",
    "        [(id_ - k) if id_ > (k-1) else id_+(len(geom_points_gdf)-k) for id_ in geom_points_gdf['PID']])\n",
    "    geom_points_gdf['b_x'] = pd.Series([geom_points_gdf['a_x'][i] for i in geom_points_gdf['b_key']])\n",
    "    geom_points_gdf['b_y'] = pd.Series([geom_points_gdf['a_y'][i] for i in geom_points_gdf['b_key']])\n",
    "    geom_points_gdf['c_key'] = pd.Series(\n",
    "        [(id_ + k) if id_ < len(geom_points_gdf)-1-(k-1) else id_-k-1 for id_ in geom_points_gdf['PID']])\n",
    "    geom_points_gdf['c_x'] = pd.Series([geom_points_gdf['a_x'][i] for i in geom_points_gdf['c_key']])\n",
    "    geom_points_gdf['c_y'] = pd.Series([geom_points_gdf['a_y'][i] for i in geom_points_gdf['c_key']])\n",
    "\n",
    "    geom_points_gdf['arc_ba'] = round(\n",
    "        1 - np.arctan2(geom_points_gdf['a_y'] - geom_points_gdf['b_y'], geom_points_gdf['a_x'] - geom_points_gdf['b_x']) / np.pi, 6)\n",
    "    geom_points_gdf['arc_ac'] = round(\n",
    "        1 - np.arctan2(geom_points_gdf['c_y'] - geom_points_gdf['a_y'], geom_points_gdf['c_x'] - geom_points_gdf['a_x']) / np.pi, 6)\n",
    "\n",
    "    geom_points_gdf['angle_bac'] = pd.Series([(ac - ba - 1) % 2 for ba, ac in geom_points_gdf[['arc_ba', 'arc_ac']].values])\n",
    "\n",
    "    cols = ['angle_bac']\n",
    "    geom_points_gdf = geom_points_gdf[[x for x in cols if x in geom_points_gdf.columns]]\n",
    "\n",
    "    \n",
    "    if normalize != 'none':\n",
    "\n",
    "        df_features = copy.deepcopy(geom_points_gdf)\n",
    "\n",
    "        df_stat = df_features[cols].describe().transpose()\n",
    "        for col in cols:\n",
    "            col_min, col_max, col_std, col_mean = df_stat.loc[col][['min', 'max', 'std', 'mean']].values\n",
    "            if normalize == 'zscore':\n",
    "                df_features[col] = (df_features[col] - col_mean) / col_std\n",
    "            elif normalize == 'minmax':\n",
    "                df_features[col] = (df_features[col] - col_min) / (col_max - col_min)\n",
    "\n",
    "        geom_points_gdf = df_features\n",
    "\n",
    "    return np.array(geom_points_gdf.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from Yan et al. 2021\n",
    "\n",
    "def get_single_features_final_new(uid, geo, k):\n",
    "    geom_points = list(geo.exterior.coords)\n",
    "    geom_points_gdf = gpd.GeoDataFrame(geom_points)\n",
    "    geom_points_gdf.columns = ['a_x', 'a_y']\n",
    "    geom_points_gdf['o_x'] = 0\n",
    "    geom_points_gdf['o_y'] = 0\n",
    "    geom_points_gdf['UID'] = uid\n",
    "    geom_points_gdf['PID'] = pd.Series([i for i in range(len(geom_points_gdf))])\n",
    "    geom_points_gdf['OID_UID'] = pd.Series([uid*1000+ pid for uid, pid in geom_points_gdf[['UID', 'PID']].values])\n",
    "    geom_points_gdf['b_key'] = pd.Series(\n",
    "        [(id_ - k) if id_ > (k-1) else id_+(len(geom_points_gdf)-k) for id_ in geom_points_gdf['PID']])\n",
    "    geom_points_gdf['b_x'] = pd.Series([geom_points_gdf['a_x'][i] for i in geom_points_gdf['b_key']])\n",
    "    geom_points_gdf['b_y'] = pd.Series([geom_points_gdf['a_y'][i] for i in geom_points_gdf['b_key']])\n",
    "    geom_points_gdf['c_key'] = pd.Series(\n",
    "        [(id_ + k) if id_ < len(geom_points_gdf)-1-(k-1) else id_-k-1 for id_ in geom_points_gdf['PID']])\n",
    "    geom_points_gdf['c_x'] = pd.Series([geom_points_gdf['a_x'][i] for i in geom_points_gdf['c_key']])\n",
    "    geom_points_gdf['c_y'] = pd.Series([geom_points_gdf['a_y'][i] for i in geom_points_gdf['c_key']])\n",
    "    geom_points_gdf['Area'] = geo.area\n",
    "\n",
    "# AB,AC,BC,OA,OB,OC\n",
    "    geom_points_gdf['l_ab'] = pd.Series(\n",
    "        [cal_euclidean([ax, ay], [bx, by]) for ax, ay, bx, by in geom_points_gdf[['a_x', 'a_y', 'b_x', 'b_y']].values])\n",
    "    geom_points_gdf['l_ac'] = pd.Series(\n",
    "        [cal_euclidean([ax, ay], [cx, cy]) for ax, ay, cx, cy in geom_points_gdf[['a_x', 'a_y', 'c_x', 'c_y']].values])\n",
    "    geom_points_gdf['l_bc'] = pd.Series(\n",
    "        [cal_euclidean([cx, cy], [bx, by]) for cx, cy, bx, by in geom_points_gdf[['c_x', 'c_y', 'b_x', 'b_y']].values])\n",
    "    geom_points_gdf['l_oa'] = pd.Series(\n",
    "        [cal_euclidean([ax, ay], [ox, oy]) for ax, ay, ox, oy in geom_points_gdf[['a_x', 'a_y', 'o_x', 'o_y']].values])\n",
    "    geom_points_gdf['l_ob'] = pd.Series(\n",
    "        [cal_euclidean([bx, by], [ox, oy]) for bx, by, ox, oy in geom_points_gdf[['b_x', 'b_y', 'o_x', 'o_y']].values])\n",
    "    geom_points_gdf['l_oc'] = pd.Series(\n",
    "        [cal_euclidean([cx, cy], [ox, oy]) for cx, cy, ox, oy in geom_points_gdf[['c_x', 'c_y', 'o_x', 'o_y']].values])\n",
    "    #\n",
    "\n",
    "    geom_points_gdf['arc_ba'] = round(\n",
    "        1 - np.arctan2(geom_points_gdf['a_y'] - geom_points_gdf['b_y'], geom_points_gdf['a_x'] - geom_points_gdf['b_x']) / np.pi, 6)\n",
    "    geom_points_gdf['arc_ac'] = round(\n",
    "        1 - np.arctan2(geom_points_gdf['c_y'] - geom_points_gdf['a_y'], geom_points_gdf['c_x'] - geom_points_gdf['a_x']) / np.pi, 6)\n",
    "    geom_points_gdf['arc_bc'] = round(\n",
    "        1 - np.arctan2(geom_points_gdf['b_y'] - geom_points_gdf['c_y'], geom_points_gdf['b_x'] - geom_points_gdf['c_x']) / np.pi, 6)\n",
    "    geom_points_gdf['arc_ob'] = round(\n",
    "        1 - np.arctan2(geom_points_gdf['b_y'] - geom_points_gdf['o_y'], geom_points_gdf['b_x'] - geom_points_gdf['o_x']) / np.pi, 6)\n",
    "    geom_points_gdf['arc_oc'] = round(\n",
    "        1 - np.arctan2(geom_points_gdf['c_y'] - geom_points_gdf['o_y'], geom_points_gdf['c_x'] - geom_points_gdf['o_x']) / np.pi, 6)\n",
    "    # ;\n",
    "    geom_points_gdf['angle_bac'] = pd.Series([(ac - ba - 1) % 2 for ba, ac in geom_points_gdf[['arc_ba', 'arc_ac']].values])\n",
    "    geom_points_gdf['angle_cba'] = pd.Series([(ba - bc - 1) % 2 for ba, bc in geom_points_gdf[['arc_ba', 'arc_bc']].values])\n",
    "    geom_points_gdf['angle_boc'] = pd.Series([(oc - ob) % 2 for ob, oc in geom_points_gdf[['arc_ob', 'arc_oc']].values])\n",
    "    geom_points_gdf['angle_cbo'] = pd.Series([(bo - bc - 1) % 2 for bo, bc in geom_points_gdf[['arc_ob', 'arc_bc']].values])\n",
    "    #\n",
    "    geom_points_gdf['angle_bac_change'] = pd.Series(\n",
    "        [(ac - ba) % 2 for ba, ac in geom_points_gdf[['arc_ba', 'arc_ac']].values])\n",
    "    geom_points_gdf['angle_bac_change'] = pd.Series(\n",
    "        [change if change <= 1 else change - 2 for change in geom_points_gdf['angle_bac_change']])\n",
    "\n",
    "    #\n",
    "    geom_points_gdf['rotate_bac'] = pd.Series([angle if angle < 1 else 2 - angle for angle in geom_points_gdf['angle_bac']])\n",
    "    geom_points_gdf['rotate_boc'] = pd.Series([angle if angle < 1 else 2 - angle for angle in geom_points_gdf['angle_boc']])\n",
    "    #\n",
    "    \n",
    "    geom_points_gdf['height_a'] = pd.Series([a*np.sin(angle_b) for a, angle_b in geom_points_gdf[['l_bc', 'angle_cba']].values])\n",
    "    geom_points_gdf['height_o'] = pd.Series([o*np.sin(angle_b) for o, angle_b in geom_points_gdf[['l_bc', 'angle_cbo']].values])\n",
    "    \n",
    "    # Area of Tri_ABC\n",
    "    geom_points_gdf['s_abc'] = pd.Series([(-1 if angle < 1 else 1) * cal_area(l1, l2, l3) for l1, l2, l3, angle in\n",
    "                                    geom_points_gdf[['l_ab', 'l_bc', 'l_ac', 'angle_bac']].values])\n",
    "    # Area of Tri_OBC\n",
    "    geom_points_gdf['s_obc'] = pd.Series([cal_area(l1, l2, l3) for l1, l2, l3 in geom_points_gdf[['l_ob', 'l_bc', 'l_oc']].values])\n",
    "    # of Tri_OBC\n",
    "    geom_points_gdf['c_obc'] = (geom_points_gdf['l_ob'] + geom_points_gdf['l_oc'] + geom_points_gdf['l_bc']) / 3\n",
    "    geom_points_gdf['r_obc'] = geom_points_gdf['s_obc'] / geom_points_gdf['c_obc']\n",
    "\n",
    "    #     #\n",
    "    geom_points_gdf['s_abc'] = pd.Series([(-1 if angle < 1 else 1) * feat / area for feat, area, angle in\n",
    "                                    geom_points_gdf[['s_abc', 'Area', 'angle_bac']].values])\n",
    "    geom_points_gdf['l_bc'] = pd.Series([(-1 if angle < 1 else 1) * feat / np.sqrt(area) for feat, area, angle in\n",
    "                                   geom_points_gdf[['l_bc', 'Area', 'angle_bac']].values])\n",
    "    geom_points_gdf['s_obc'] = pd.Series([feat / area for feat, area in geom_points_gdf[['s_obc', 'Area']].values])\n",
    "    geom_points_gdf['c_obc'] = pd.Series([feat / np.sqrt(area) for feat, area in geom_points_gdf[['c_obc', 'Area']].values])\n",
    "    geom_points_gdf['r_obc'] = pd.Series([feat / np.sqrt(area) for feat, area in geom_points_gdf[['r_obc', 'Area']].values])\n",
    "    geom_points_gdf['l_oa'] = pd.Series([feat / np.sqrt(area) for feat, area in geom_points_gdf[['l_oa', 'Area']].values])\n",
    "\n",
    "    cols = ['OBJECTID', 'PID', 'UID', 'OID_UID', 'isBegin', 'isStart', 'Area'\n",
    "        , 'l_bc', 'l_oa', 'l_ab', 'l_ac', 'l_ob', 'l_oc'\n",
    "        , 'rotate_bac', 'rotate_boc'\n",
    "        , 'angle_bac_change', 'angle_bac', 'angle_boc'\n",
    "        , 's_abc', 's_obc', 'c_obc', 'r_obc'\n",
    "        , 'height_a', 'height_o'\n",
    "            ]\n",
    "    geom_points_gdf = geom_points_gdf[[x for x in cols if x in geom_points_gdf.columns]]\n",
    "    return geom_points_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from Yan et al. 2021\n",
    "\n",
    "def get_normalize_features_final(df_features, norm_type):\n",
    "    df_features = copy.deepcopy(df_features)\n",
    "    cols = [x for x in df_features.columns if 'k' in x and 'rotate' not in x]\n",
    "    #\n",
    "    df_stat = df_features[cols].describe().transpose()\n",
    "    for col in cols:\n",
    "        col_min, col_max, col_std, col_mean = df_stat.loc[col][['min', 'max', 'std', 'mean']].values\n",
    "        if norm_type == 'zscore':\n",
    "            df_features[col] = (df_features[col] - col_mean) / col_std\n",
    "        elif norm_type == 'minmax':\n",
    "            df_features[col] = (df_features[col] - col_min) / (col_max - col_min)\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from Yan et al. 2021\n",
    "\n",
    "def cal_euclidean(p1, p2):\n",
    "    return np.linalg.norm([p1[0] - p2[0], p1[1] - p2[1]])\n",
    "\n",
    "def cal_area(l1, l2, l3):\n",
    "    p = (l1 + l2 + l3) / 2\n",
    "    area = p * (p - l1) * (p - l2) * (p - l3)\n",
    "    area = 0 if area <= 0 else np.sqrt(area)\n",
    "    # area=np.sqrt(p*(p-l1)*(p-l2)*(p-l3))\n",
    "    return area\n",
    "\n",
    "def cal_arc(p1, p2, degree=False):\n",
    "    dx, dy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    arc = np.pi - np.arctan2(dy, dx)\n",
    "    return arc / np.pi * 180 if degree else arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tensors(buildings, encoding_schemes, k_list,\n",
    "                  local_features, regional_features, global_features_polygon,\n",
    "                  intermediate = True, id_name = 'OBJECTID'):\n",
    "    '''\n",
    "        buildings: list of building geometries\n",
    "        shape_dict: dict for class names\n",
    "        encoding_schemes: encoding scheme    '2': (x, y)  = default\n",
    "                                            '3': (x,y, 1) (single one-hot-vector for intermediate/end point) \n",
    "                                            '4': (x,y, 1, 0) (double one-hot-vector for intermediate/end point)\n",
    "                                            '5': (x,y, 1, 0, 0) (triple one-hot-vector for intermediate/end point)\n",
    "                                            'f': feature encoding as of Yan et al. 2021\n",
    "                                            2f: combining 2 + f\n",
    "                                            5f: combining 5+f\n",
    "        local_features, regional_features, global_features_polygon: selected features\n",
    "        intermediate: encoding value of intermediate points (1, 0, 0) or (0, 1, 1)\n",
    "        id_name: id attribute in buildings properties\n",
    "     '''\n",
    "    \n",
    "    print('Start encoding features at ' + str(datetime.now()))\n",
    "    \n",
    "    print(encoding_schemes)\n",
    "    df_buildings = copy.deepcopy(buildings)\n",
    "    if id_name != 'OBJECTID':\n",
    "        df_buildings['OBJECTID'] = pd.Series([int(oid) for oid in df_buildings[id_name]], dtype=int)\n",
    "    df_buildings.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    scale = 1\n",
    "    \n",
    "    cols = ['enc_{}'.format(x) for x in encoding_schemes]\n",
    "    #print(cols)\n",
    "    \n",
    "    for c in cols:\n",
    "             \n",
    "        if c == 'enc_2':\n",
    "            df_buildings[c] = pd.Series([encoding_2(geo, centroid, scale) \n",
    "                                      for geo, centroid in df_buildings[['geometry', 'centroid']].values])\n",
    "            print('       Done encoding 2D-features at ' + str(datetime.now()))\n",
    "                \n",
    "        if c == 'enc_3':\n",
    "            df_buildings[c] = pd.Series([encoding_3(geo, centroid, scale) \n",
    "                                         for geo, centroid in df_buildings[['geometry', 'centroid']].values])\n",
    "            print('       Done encoding 3D-features at ' + str(datetime.now()))\n",
    "            \n",
    "        if c == 'enc_4':\n",
    "            df_buildings[c] = pd.Series([encoding_4(geo, centroid, scale) \n",
    "                                         for geo, centroid in df_buildings[['geometry', 'centroid']].values])\n",
    "            print('       Done encoding 4D-features at ' + str(datetime.now()))\n",
    "\n",
    "        if c == 'enc_5':\n",
    "            df_buildings[c] = pd.Series([encoding_5(geo, centroid, scale) \n",
    "                                         for geo, centroid in df_buildings[['geometry', 'centroid']].values])\n",
    "            print('       Done encoding 5D-features at ' + str(datetime.now()))\n",
    "        \n",
    "        if c == 'enc_f':\n",
    "            df_buildings[c] = pd.Series([encoding_graph_features(uid, geo, local_features, regional_features, global_features_polygon, klst = k_list) \n",
    "                                         for uid, geo in df_buildings[['OBJECTID', 'geometry']].values])\n",
    "            print('       Done encoding graph features at ' + str(datetime.now()))\n",
    "        \n",
    "        if c == 'enc_graph_2':\n",
    "            df_buildings[c] = pd.Series([encoding_graph2_features(uid, geo) \n",
    "                                         for uid, geo in df_buildings[['OBJECTID', 'geometry']].values])\n",
    "            print('       Done encoding 2D-graph features at ' + str(datetime.now()))\n",
    "        \n",
    "        if c == 'enc_2f':\n",
    "            df_buildings[c] = pd.Series([np.concatenate((a, b), axis=1) \n",
    "                                         for a, b in df_buildings[['enc_2','enc_f']].values])\n",
    "            print('       Done encoding 2f-graph features at ' + str(datetime.now()))\n",
    "            \n",
    "        if c == 'enc_5f':\n",
    "            df_buildings[c] = pd.Series([np.concatenate((a, b), axis=1) \n",
    "                                         for a, b in df_buildings[['enc_5','enc_f']].values])\n",
    "            print('       Done encoding 5f-graph features at ' + str(datetime.now()))\n",
    "        \n",
    "    print('Done encoding features at ' + str(datetime.now()))\n",
    "    \n",
    "    return df_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test_predict(model, test_data_loader, target_dict, transpose = False, graph = False):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    model: model\n",
    "    test_data_loader: test data\n",
    "    target_dict: dict to translate target values to class name\n",
    "    \n",
    "    output:\n",
    "    targets_list: list of target values\n",
    "    targets_shape: list of translated target values\n",
    "    predictions: list of predictions\n",
    "    shape_predictions: list of translated predictions\n",
    "    embeds: embedding state\n",
    "    \"\"\" \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        targets_list = []\n",
    "        targets_shape = []\n",
    "        predictions = []\n",
    "        shape_predictions = []\n",
    "\n",
    "        embeds = []\n",
    "\n",
    "        for batch in test_data_loader:  \n",
    "            input_, targets = batch\n",
    "            \n",
    "            if transpose == True:\n",
    "                try:\n",
    "                    outputs, embedding = model(torch.transpose(input_, 1, 2).float())\n",
    "                except Exception:\n",
    "                    outputs, _, embedding = model(torch.transpose(input_, 1, 2).float())\n",
    "            elif graph == True:\n",
    "                outputs, embedding = model(input_)\n",
    "            else:\n",
    "                outputs, embedding = model(input_.float())\n",
    "            embeds.append([targets.float(), embedding])\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            for t in targets:\n",
    "                targets_list.append(int(t))\n",
    "                targets_shape.append(shape_dict_reverse[str(int(t))])\n",
    "            for p in predicted:\n",
    "                predictions.append(int(p))\n",
    "                shape_predictions.append(shape_dict_reverse[str(int(p))])\n",
    "\n",
    "        print('Test Accuracy of the model on the test polygons: {} %'.format(100 * correct / total))\n",
    "    \n",
    "    return targets_list, targets_shape, predictions, shape_predictions, embeds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Visualization\n",
    "def embedding_viz(embeddings, folder_filename, n_components=2, perplexity=30.0, learning_rate=200, init='random'):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    embeddings: embeddings states\n",
    "    folder_filename: folder + filename\n",
    "    TSNE parameters:\n",
    "        n_components\n",
    "        perplexity\n",
    "        learning_rate\n",
    "        init\n",
    "    \n",
    "    saves figure under folder_filename and shows plot\n",
    "    \"\"\" \n",
    "    first = True\n",
    "    x = []\n",
    "    y = []\n",
    "    cols = []\n",
    "\n",
    "    for batch in embeddings:\n",
    "        targets = batch[0].detach().numpy()\n",
    "        for t in targets:\n",
    "            cols.append(t)\n",
    "        ems = batch[1].detach().numpy()\n",
    "        if first == True:\n",
    "            embedding = ems\n",
    "            first = False\n",
    "        else:\n",
    "            embedding = np.append(embedding, ems, axis = 0)\n",
    "\n",
    "    embedding_ = copy.deepcopy(embedding)\n",
    "    #print(embedding.shape)\n",
    "    \n",
    "    X_embedded = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate, init=init).fit_transform(embedding_)\n",
    "\n",
    "    for e in X_embedded:\n",
    "        x.append(e[0])\n",
    "        y.append(e[1])\n",
    "\n",
    "    plt.scatter(x, y, s=3, c = cols, cmap = 'tab10')\n",
    "    plt.savefig(folder_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_statistics(targets_list, predictions_list, target_selection, target_dict):\n",
    "    \n",
    "    keys = [target_dict[str(i)] for i in target_selection]\n",
    "    \n",
    "    report = classification_report(targets_list, predictions_list, output_dict=True)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    support = []\n",
    "\n",
    "    for i in target_selection:\n",
    "        precision.append(report[str(i)]['precision'])\n",
    "        recall.append(report[str(i)]['recall'])\n",
    "        f1_score.append(report[str(i)]['f1-score'])\n",
    "        support.append(report[str(i)]['support'])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (15,5))\n",
    "\n",
    "    ax[0].plot(keys, precision, label = 'precision')\n",
    "    ax[0].plot(keys, recall, label = 'recall')\n",
    "    ax[0].plot(keys, f1_score, label = 'f1-score')\n",
    "\n",
    "    ax[0].set_xlabel('shape')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title('Summary')\n",
    "\n",
    "    ax[1].plot(keys, support, label = 'support')\n",
    "    ax[1].set_xlabel('shape')\n",
    "    ax[1].set_ylabel('occurences')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title('Support')\n",
    "\n",
    "    return precision, recall, f1_score, support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix(predictions_df, target_selection):\n",
    "    matrix = []\n",
    "    for shapes in target_selection:\n",
    "        subset = predictions_df[predictions_df['target'] == shapes]\n",
    "        shape_list = []\n",
    "        for s in target_selection:\n",
    "            subsubset = subset[predictions_df['clas_prediction'] == s]\n",
    "            shape_list.append(len(subsubset)/len(subset))\n",
    "        matrix.append(shape_list)\n",
    "    \n",
    "    f, ax = plt.subplots(figsize = (8,8))\n",
    "\n",
    "    ax.matshow(np.array(matrix),cmap=plt.cm.Blues)\n",
    "    ax.set_xticks([i for i in range(9)])\n",
    "    ax.set_xticklabels([i for i in ('E', 'F', 'H', 'I', 'L', 'O', 'T', 'U', 'Z')])\n",
    "    ax.set_yticks([i for i in range(9)])\n",
    "    ax.set_yticklabels([i for i in ('E', 'F', 'H', 'I', 'L', 'O', 'T', 'U', 'Z')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'path to folder for results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "imagefolder = folder+ 'img/'\n",
    "if not os.path.exists(imagefolder):\n",
    "    os.makedirs(imagefolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "hp = {\n",
    "    'train_test_split': 0.8,\n",
    "    'shuffled_train': True,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'rec_dropout': 0.3,    \n",
    "    'input_size': 64,\n",
    "    'num_classes': 10,  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = {\n",
    "    'normalize': True,\n",
    "    'mode': 'iterativ', # 'iterativ', 'interpolate' or 'padding'\n",
    "    'k_list': [1,2],\n",
    "    'scaling': 'x', # 'none', 'x', 'y' or 'xy'\n",
    "    'scale_method': 'minmax', # 'minmax' or 'stdev' \n",
    "    'centroid': 'geometry', # 'geometry' or 'map'\n",
    "    'sampling': 'none', # 'none', 'rotate' or 'random'\n",
    "    'samples': 1, # assert >0\n",
    "    'rotate_to_y_axis': 'none', # 'none', 'all', 'train' or 'test'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_schemes = ['2', '5', 'f', '2f', '5f']\n",
    "test_selection = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "local_features = [#'triangle_area', # Area of the triangle ABC\n",
    "                   'len_c', # Length of adjacent line BA\n",
    "                   'len_b', # Length of adjacent line AC\n",
    "                   'angle_a', # Turning angle at point A\n",
    "                   'len_a', # Length of opposite line from point A (BC)\n",
    "                   #'height', # Height of the triangle ABC\n",
    "                  ]\n",
    "\n",
    "regional_features = [#'reg_triangle_area', # Area of the triangle OBC\n",
    "                   'reg_len_c', # Length of adjacent line BO\n",
    "                   'reg_len_b', # Length of adjacent line OC\n",
    "                   'reg_angle_o', # Turning angle at point O\n",
    "                   #'reg_height', # Height of the triangle OBC\n",
    "                   'semiperimeter', # semi_perimeter (BO+OC+BC)/3\n",
    "                   'radius', # 'reg_triangle_area' / 'semiperimeter'\n",
    "                  ]\n",
    "\n",
    "\n",
    "global_features_polygon = [#'area', # Area of the polygon\n",
    "                           #'mabr', # Minimum area bounding rectangle\n",
    "                           'elongation', # Length to width ratio of the MABR\n",
    "                           'circularity', # Deviation between polygon and its equal-perimeter circle.\n",
    "                           'rectangularity', # Deviation between polygon and its MABR\n",
    "                           'squareness', # Deviation between polygon and its equal-area square\n",
    "                           'convexity', # Deviation between polygon and its convex hull\n",
    "                           #'fractality', # Edge roughness or smoothness\n",
    "                           #'orientation', # Angle between major axis of the MABR and the horizontal direction\n",
    "                           'mean_dist_o', # Mean distance from vertices to centroid\n",
    "                          ]\n",
    "\n",
    "global_features_line = [#'l_mabr', # The minimum area bounding rectangle of the line\n",
    "                        #'l_mabr_area', # The area of the MABR of the line\n",
    "                        #'len_anchor_line', # The length of the MABR\n",
    "                        #'bandwidth', # The width of the MABR\n",
    "                        #'segmentation', # Distance from the beginning to the location on the anchor line where the maximum\n",
    "                        #'width_pos', # Maximum deviation on one side of the anchor line\n",
    "                        #'width_neg', # Maximum deviation on other side of the anchor line\n",
    "                        #'concurrence', # Number of times line crosses anchor line\n",
    "                        #'error_variance', # Discrete approximation of total discrepancy between line and anchor line\n",
    "                        #'bendings', # Number of bends\n",
    "                        #'sinuosity', # Number of inflection points\n",
    "                        #'directionality', # Deviation between line length and anchor line length\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "for key, value in hp.items():\n",
    "    text_file.write(key + ': ' + str(value) + '\\n')\n",
    "\n",
    "text_file.write('\\n')\n",
    "text_file.write('experiment parameters:' + '\\n')\n",
    "for key, value in exp.items():\n",
    "    text_file.write(key + ': ' + str(value) + '\\n')\n",
    "\n",
    "text_file.write('\\n')\n",
    "text_file.write('Encoding_schemes: ' + str(encodings_schemes))\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_shapes = gpd.read_file('path to file') # File from Yan et al. 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_dict = {'E' : 0, 'F' : 1, 'H' : 2, 'I' : 3, 'L' : 4, 'O' : 5, 'T' : 6, 'U' : 7, 'Y' : 8, 'Z' : 9}\n",
    "shape_dict_reverse = {'0': 'E', '1': 'F', '2': 'H', '3': 'I', '4': 'L', '5': 'O', '6': 'T', '7': 'U', '8': 'Y', '9': 'Z'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start preprocessing at ' + str(datetime.now()))\n",
    "\n",
    "buildings_shapes = preprocess_geometries(buildings_shapes, shape_dict, hp['input_size'], centroid_type = exp['centroid'], mode = exp['mode'])\n",
    "\n",
    "if exp['normalize']:\n",
    "    if exp['centroid'] == 'geometry':\n",
    "        buildings_shapes = normalize_shape_geometry(buildings_shapes, scaling = exp['scaling'], scale_type = exp['scale_method'])\n",
    "    elif exp['centroid'] == 'map':\n",
    "        buildings_shapes = normalize_shape_map(buildings_shapes, scaling = exp['scaling'], scale_type = exp['scale_method'])\n",
    "\n",
    "# Train-Test-Split\n",
    "train_buildings, test_buildings = train_test_split(buildings_shapes, train_size = hp['train_test_split'])\n",
    "train_buildings['status'] = 'train'\n",
    "test_buildings['status'] = 'test'\n",
    "building_data = pd.concat([train_buildings, test_buildings])\n",
    "building_data.sort_index(inplace=True)\n",
    "\n",
    "print('End preprocessing at ' + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp['sampling'] != 'none':\n",
    "    building_data = rotate_geometries(building_data, sampling = exp['sampling'], samples = exp['samples'])\n",
    "\n",
    "if exp['rotate_to_y_axis'] != 'none':\n",
    "    building_data = rotate_geom_to_y_axis(building_data, which_data = exp['rotate_to_y_axis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "building_data = write_tensors(building_data, encodings_schemes, exp['k_list'],\n",
    "                              local_features, regional_features, global_features_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp['mode'] == 'padding':\n",
    "    building_data['enc_2'] = pd.Series([np.concatenate((enc_2, np.zeros([encod_len-orig_len, 2])), axis=0) \n",
    "                                        for enc_2, encod_len, orig_len in building_data[['enc_2', 'encod_len', 'orig_len']].values])\n",
    "    building_data['enc_5'] = pd.Series([np.concatenate((enc_5, np.zeros([encod_len-orig_len, 5])), axis=0) \n",
    "                                        for enc_5, encod_len, orig_len in building_data[['enc_5', 'encod_len', 'orig_len']].values])\n",
    "    building_data['enc_f'] = pd.Series([np.concatenate((enc_f, np.zeros([encod_len-orig_len, enc_f.shape[1]])), axis=0) \n",
    "                                        for enc_f, encod_len, orig_len in building_data[['enc_f', 'encod_len', 'orig_len']].values])\n",
    "    building_data['enc_2f'] = pd.Series([np.concatenate((enc_2f, np.zeros([encod_len-orig_len, enc_2f.shape[1]])), axis=0) \n",
    "                                        for enc_2f, encod_len, orig_len in building_data[['enc_2f', 'encod_len', 'orig_len']].values])\n",
    "    building_data['enc_5f'] = pd.Series([np.concatenate((enc_5f, np.zeros([encod_len-orig_len, enc_5f.shape[1]])), axis=0) \n",
    "                                        for enc_5f, encod_len, orig_len in building_data[['enc_5f', 'encod_len', 'orig_len']].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buildings = building_data[building_data['status'] == 'train']\n",
    "test_buildings = building_data[building_data['status'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_buildings.iloc[0]['enc_5f'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_buildings.hist(column=[\"orig_len\"], bins= 80, figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Start Data Loader processing at ' + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geoms_train = train_buildings['enc_2'].tolist()\n",
    "data_targets_train = train_buildings['shape_char'].tolist()\n",
    "data_len_train = train_buildings['encod_len'].tolist()\n",
    "\n",
    "data_geoms_test = test_buildings['enc_2'].tolist()\n",
    "data_targets_test = test_buildings['shape_char'].tolist()\n",
    "data_len_test = test_buildings['encod_len'].tolist()\n",
    "\n",
    "data_geoms_train_5 = train_buildings['enc_5'].tolist()\n",
    "data_geoms_test_5 = test_buildings['enc_5'].tolist()\n",
    "\n",
    "data_geoms_train_G = train_buildings['enc_f'].tolist()\n",
    "data_geoms_test_G = test_buildings['enc_f'].tolist()\n",
    "\n",
    "data_geoms_train_G2 = train_buildings['enc_2f'].tolist()\n",
    "data_geoms_test_G2 = test_buildings['enc_2f'].tolist()\n",
    "\n",
    "data_geoms_train_G5 = train_buildings['enc_5f'].tolist()\n",
    "data_geoms_test_G5 = test_buildings['enc_5f'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_c = OrderedSeqs_Dataset(data_geoms_train, data_targets_train)\n",
    "test_dataset_c = OrderedSeqs_Dataset(data_geoms_test, data_targets_test)\n",
    "\n",
    "train_dataset_s = OrderedSeqs_Dataset(data_geoms_train_5, data_targets_train)\n",
    "test_dataset_s = OrderedSeqs_Dataset(data_geoms_test_5, data_targets_test)\n",
    "\n",
    "train_dataset_f = OrderedSeqs_Dataset(data_geoms_train_G, data_targets_train)\n",
    "test_dataset_f = OrderedSeqs_Dataset(data_geoms_test_G, data_targets_test)\n",
    "\n",
    "train_dataset_2f = OrderedSeqs_Dataset(data_geoms_train_G2, data_targets_train)\n",
    "test_dataset_2f = OrderedSeqs_Dataset(data_geoms_test_G2, data_targets_test)\n",
    "\n",
    "train_dataset_5f = OrderedSeqs_Dataset(data_geoms_train_G5, data_targets_train)\n",
    "test_dataset_5f = OrderedSeqs_Dataset(data_geoms_test_G5, data_targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Train Dataset:\", len(train_dataset_c))\n",
    "print(\"Size of Test Dataset:\", len(test_dataset_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader_c = data.DataLoader(train_dataset_c, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "data_inputs_c, data_labels_c = next(iter(train_data_loader_c))\n",
    "\n",
    "train_data_loader_s = data.DataLoader(train_dataset_s, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "data_inputs_s, data_labels_s = next(iter(train_data_loader_s))\n",
    "\n",
    "train_data_loader_f = data.DataLoader(train_dataset_f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "data_inputs_f, data_labels_f = next(iter(train_data_loader_f))\n",
    "\n",
    "train_data_loader_2f = data.DataLoader(train_dataset_2f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "data_inputs_2f, data_labels_2f = next(iter(train_data_loader_2f))\n",
    "\n",
    "train_data_loader_5f = data.DataLoader(train_dataset_5f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "data_inputs_5f, data_labels_5f = next(iter(train_data_loader_5f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader_c = data.DataLoader(test_dataset_c, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "test_data_loader_s = data.DataLoader(test_dataset_s, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "test_data_loader_f = data.DataLoader(test_dataset_f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "test_data_loader_2f = data.DataLoader(test_dataset_2f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "test_data_loader_5f = data.DataLoader(test_dataset_5f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs_5f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs_c.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_graph_c = Graph_Dataset(data_geoms_train, data_targets_train, data_len_train)\n",
    "test_dataset_graph_c = Graph_Dataset(data_geoms_test, data_targets_test, data_len_test)\n",
    "\n",
    "train_dataset_graph_s = Graph_Dataset(data_geoms_train_5, data_targets_train, data_len_train)\n",
    "test_dataset_graph_s = Graph_Dataset(data_geoms_test_5, data_targets_test, data_len_test)\n",
    "\n",
    "train_dataset_graph_f = Graph_Dataset(data_geoms_train_G, data_targets_train, data_len_train)\n",
    "test_dataset_graph_f = Graph_Dataset(data_geoms_test_G, data_targets_test, data_len_test)\n",
    "\n",
    "train_dataset_graph_2f = Graph_Dataset(data_geoms_train_G2, data_targets_train, data_len_train)\n",
    "test_dataset_graph_2f = Graph_Dataset(data_geoms_test_G2, data_targets_test, data_len_test)\n",
    "\n",
    "train_dataset_graph_5f = Graph_Dataset(data_geoms_train_G5, data_targets_train, data_len_train)\n",
    "test_dataset_graph_5f = Graph_Dataset(data_geoms_test_G5, data_targets_test, data_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Train Dataset:\", len(train_dataset_graph_c))\n",
    "print(\"Size of Test Dataset:\", len(test_dataset_graph_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader_graph_c = data.DataLoader(train_dataset_graph_c, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "data_inputs_graph_c, data_labels_graph_c = next(iter(train_data_loader_graph_c))\n",
    "\n",
    "train_data_loader_graph_s = data.DataLoader(train_dataset_graph_s, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "data_inputs_graph_s, data_labels_graph_s = next(iter(train_data_loader_graph_s))\n",
    "\n",
    "train_data_loader_graph_f = data.DataLoader(train_dataset_graph_f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "data_inputs_graph_f, data_labels_graph_f = next(iter(train_data_loader_graph_f))\n",
    "\n",
    "train_data_loader_graph_2f = data.DataLoader(train_dataset_graph_2f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "data_inputs_graph_2f, data_labels_graph_2f = next(iter(train_data_loader_graph_2f))\n",
    "\n",
    "train_data_loader_graph_5f = data.DataLoader(train_dataset_graph_5f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "data_inputs_graph_5f, data_labels_graph_5f = next(iter(train_data_loader_graph_5f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader_graph_c = data.DataLoader(test_dataset_graph_c, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "test_data_loader_graph_s = data.DataLoader(test_dataset_graph_s, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "test_data_loader_graph_f = data.DataLoader(test_dataset_graph_f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "test_data_loader_graph_2f = data.DataLoader(test_dataset_graph_2f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "test_data_loader_graph_5f = data.DataLoader(test_dataset_graph_5f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs_graph_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs_graph_c.ndata['x'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('End Data Loader processing at ' + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tVeerCNN+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tVeerCNNc = folder+'tVeerCNNc/'\n",
    "if not os.path.exists(folder_tVeerCNNc):\n",
    "    os.makedirs(folder_tVeerCNNc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tVeerCNNc_dims = {2: {'sequence_length': 2,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                3: {'sequence_length': 3,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                4: {'sequence_length': 4,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2},\n",
    "                5: {'sequence_length': 5,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_tVeerCNNc+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "#text_file.write('dense_size: ' + str(dense_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "text_file.write('tVeerCNNc_dims:' + '\\n')\n",
    "for key, value in tVeerCNNc_dims[sequence_length].items():\n",
    "    text_file.write(key + ': ' + str(value) + '\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class tVeerCNNc(nn.Module):\n",
    "    def __init__(self, input_size, dropout, num_classes):\n",
    "        super(tVeerCNNc, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1d_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_size, \n",
    "                      out_channels=32, \n",
    "                      kernel_size = tVeerCNNc_dims[sequence_length]['sequence_length'], \n",
    "                      padding=tVeerCNNc_dims[sequence_length]['conv1d_1_padding']),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        )\n",
    "        \n",
    "        self.conv1d_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels=64, kernel_size = 1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size = tVeerCNNc_dims[sequence_length]['sequence_length'], \n",
    "                         padding=tVeerCNNc_dims[sequence_length]['conv1d_2_padding'])\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense_1 = nn.Linear(64, 32)\n",
    "        self.dense_2 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1d_1(x)\n",
    "        out = self.conv1d_2(out)\n",
    "        embeddings = out\n",
    "        \n",
    "        B, N, C = out.shape\n",
    "        out = out.reshape([B, N*C])\n",
    "        \n",
    "        out = self.dense_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense_2(out)\n",
    "        return out, embeddings\n",
    "        \n",
    "        \n",
    "model = tVeerCNNc(hp['input_size'], hp['dropout'], hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], hp['input_size'], sequence_length), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train the CNN_paper model\n",
    "\n",
    "print('Start training of tVeerCNNc at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_tVeerCNNc = []\n",
    "test_loss_tVeerCNNc = []\n",
    "test_accuracy_tVeerCNNc = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_c = data.DataLoader(train_dataset_c, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_c:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs.float()\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_tVeerCNNc.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_c:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_.float()\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_tVeerCNNc.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_tVeerCNNc.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1,\n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_tVeerCNNc+\"model_tVeerCNNc_best.mod\")\n",
    "    \n",
    "print('End training of tVeerCNNc at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_tVeerCNNc+\"model_tVeerCNNc_best.mod\")\n",
    "\n",
    "print('Best epoch of tVeerCNNc: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n",
    "           folder_tVeerCNNc+\"model_tVeerCNNc.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_tVeerCNNc+\"model_tVeerCNNc_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_tVeerCNNc, shape_predictions_tVeerCNNc, embeds = model_test_predict(model, test_data_loader_c, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding encoding\n",
    "emb = []\n",
    "for e in embeds:\n",
    "    #print(e[1][:, :, -1])\n",
    "    emb.append((e[0], e[1][:, :, -1]))\n",
    "embeddings = emb\n",
    "\n",
    "embedding_viz(embeddings, imagefolder+'tVeerCNNc.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "targets_list, targets_shape, predictions_tVeerCNNc, shape_predictions_tVeerCNNc, embeds = model_test_predict(model, test_data_loader_c, shape_dict_reverse)\n",
    "\n",
    "pred_data_tVeerCNNc = {'index': [i for i in range(len(predictions_tVeerCNNc))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_tVeerCNNc,\n",
    "        'shape_prediction': shape_predictions_tVeerCNNc}\n",
    "predictionsDF_tVeerCNNc = pd.DataFrame(pred_data_tVeerCNNc)\n",
    "predictionsDF_tVeerCNNc.to_csv(folder_tVeerCNNc+'predictions_tVeerCNNc.csv')\n",
    "\n",
    "precision_tVeerCNNc, recall_tVeerCNNc, f1_score_tVeerCNNc, support_tVeerCNNc = report_statistics(targets_list, predictions_tVeerCNNc, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_accuracy_tVeerCNNc = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_tVeerCNNc[predictionsDF_tVeerCNNc['target'] == shapes]\n",
    "    clas_accuracy_tVeerCNNc.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_tVeerCNNc, test_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tVeerCNN+s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tVeerCNNs = folder+'tVeerCNNs/'\n",
    "if not os.path.exists(folder_tVeerCNNs):\n",
    "    os.makedirs(folder_tVeerCNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "dense_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tVeerCNNs_dims = {2: {'sequence_length': 2,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                3: {'sequence_length': 3,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                4: {'sequence_length': 4,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2},\n",
    "                5: {'sequence_length': 5,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_tVeerCNNs+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "text_file.write('dense_size: ' + str(dense_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "text_file.write('tVeerCNNs_dims:' + '\\n')\n",
    "for key, value in tVeerCNNs_dims[sequence_length].items():\n",
    "    text_file.write(key + ': ' + str(value) + '\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class tVeerCNNs(nn.Module):\n",
    "    def __init__(self, input_size, dropout, dense_size, num_classes):\n",
    "        super(tVeerCNNs, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1d_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_size, \n",
    "                      out_channels=32, \n",
    "                      kernel_size = tVeerCNNs_dims[sequence_length]['sequence_length'], \n",
    "                      padding=tVeerCNNs_dims[sequence_length]['conv1d_1_padding']),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        )\n",
    "        \n",
    "        self.conv1d_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels=64, kernel_size = 1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size = tVeerCNNs_dims[sequence_length]['sequence_length'], \n",
    "                         padding=tVeerCNNs_dims[sequence_length]['conv1d_2_padding'])\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense_1 = nn.Linear(64, dense_size)\n",
    "        self.dense_2 = nn.Linear(dense_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1d_1(x)\n",
    "        out = self.conv1d_2(out)\n",
    "        embeddings = out\n",
    "        \n",
    "        B, N, C = out.shape\n",
    "        out = out.reshape([B, N*C])\n",
    "        \n",
    "        out = self.dense_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense_2(out)\n",
    "        return out, embeddings\n",
    "        \n",
    "        \n",
    "model = tVeerCNNs(hp['input_size'], hp['dropout'], dense_size, hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], hp['input_size'], sequence_length), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train the tVeerCNNs model\n",
    "\n",
    "print('Start training of tVeerCNNs at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_tVeerCNNs = []\n",
    "test_loss_tVeerCNNs = []\n",
    "test_accuracy_tVeerCNNs = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_s = data.DataLoader(train_dataset_s, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_s:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs.float()\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_tVeerCNNs.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_s:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_.float()\n",
    "        \n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_tVeerCNNs.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_tVeerCNNs.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1,\n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_tVeerCNNs+\"model_tVeerCNNs_best.mod\")\n",
    "\n",
    "print('End training of tVeerCNNs at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_tVeerCNNs+\"model_tVeerCNNs_best.mod\")\n",
    "\n",
    "print('Best epoch of tVeerCNNs: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, \n",
    "           folder_tVeerCNNs+\"model_tVeerCNNs.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_tVeerCNNs+\"model_tVeerCNNs_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_tVeerCNNs, shape_predictions_tVeerCNNs, embeds = model_test_predict(model, test_data_loader_s, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Embedding encoding\n",
    "emb = []\n",
    "for e in embeds:\n",
    "    #print(e[1][:, :, -1])\n",
    "    emb.append((e[0], e[1][:, :, -1]))\n",
    "embeddings = emb\n",
    "\n",
    "embedding_viz(embeddings, imagefolder+'tVeerCNNs.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "targets_list, targets_shape, predictions_tVeerCNNs, shape_predictions_tVeerCNNs, embeds = model_test_predict(model, test_data_loader_s, shape_dict_reverse)\n",
    "\n",
    "pred_data_tVeerCNNs = {'index': [i for i in range(len(predictions_tVeerCNNs))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_tVeerCNNs,\n",
    "        'shape_prediction': shape_predictions_tVeerCNNs}\n",
    "predictionsDF_tVeerCNNs = pd.DataFrame(pred_data_tVeerCNNs)\n",
    "predictionsDF_tVeerCNNs.to_csv(folder_tVeerCNNs+'predictions_tVeerCNNs.csv')\n",
    "#predictionsDF_hh_cnn_paper.hist(column=[\"clas_prediction\"], figsize=(10, 8))\n",
    "\n",
    "precision_tVeerCNNs, recall_tVeerCNNs, f1_score_tVeerCNNs, support_tVeerCNNs = report_statistics(targets_list, predictions_tVeerCNNs, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_tVeerCNNs = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_tVeerCNNs[predictionsDF_tVeerCNNs['target'] == shapes]\n",
    "    clas_accuracy_tVeerCNNs.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_tVeerCNNs, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tVeerCNN+f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tVeerCNNf = folder+'tVeerCNNf/'\n",
    "if not os.path.exists(folder_tVeerCNNf):\n",
    "    os.makedirs(folder_tVeerCNNf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = data_inputs_5f.shape[2]\n",
    "print(sequence_length)\n",
    "dense_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tVeerCNNf_dims = {2: {'sequence_length': 2,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                3: {'sequence_length': 3,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                4: {'sequence_length': 4,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2},\n",
    "                5: {'sequence_length': 5,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2},\n",
    "                 20: {'sequence_length': 20,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 0},\n",
    "                 22: {'sequence_length': 22,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 0},\n",
    "                 24: {'sequence_length': 24,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 0},\n",
    "                 25: {'sequence_length': 25,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 0},\n",
    "                 26: {'sequence_length': 26,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 0},\n",
    "                 29: {'sequence_length': 29,\n",
    "                    'conv1d_1_padding': 1,\n",
    "                    'conv1d_2_padding': 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_tVeerCNNf+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "text_file.write('dense_size: ' + str(dense_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "text_file.write('tVeerCNNf_dims:' + '\\n')\n",
    "for key, value in tVeerCNNf_dims[sequence_length].items():\n",
    "    text_file.write(key + ': ' + str(value) + '\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class tVeerCNNf(nn.Module):\n",
    "    def __init__(self, input_size, dropout, dense_size, num_classes):\n",
    "        super(tVeerCNNf, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1d_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_size, \n",
    "                      out_channels=32, \n",
    "                      kernel_size = tVeerCNNf_dims[sequence_length]['sequence_length'], \n",
    "                      padding=tVeerCNNf_dims[sequence_length]['conv1d_1_padding']),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3)#, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.conv1d_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels=64, kernel_size = 1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size = 1,\n",
    "                        )\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense_1 = nn.Linear(64, 32)\n",
    "        self.dense_2 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1d_1(x)\n",
    "        out = self.conv1d_2(out)\n",
    "        embeddings = out\n",
    "        \n",
    "        B, N, C = out.shape\n",
    "        out = out.reshape([B, N*C])\n",
    "        \n",
    "        out = self.dense_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense_2(out)\n",
    "        return out, embeddings\n",
    "        \n",
    "        \n",
    "model = tVeerCNNf(hp['input_size'], hp['dropout'], dense_size, hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], hp['input_size'], sequence_length), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the tVeerCNNf model\n",
    "\n",
    "print('Start training of tVeerCNNf at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_tVeerCNNf = []\n",
    "test_loss_tVeerCNNf = []\n",
    "test_accuracy_tVeerCNNf = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_5f = data.DataLoader(train_dataset_5f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_5f:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs.float()\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_tVeerCNNf.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_5f:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_.float()\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_tVeerCNNf.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_tVeerCNNf.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1,\n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_tVeerCNNf+\"model_tVeerCNNf_best.mod\")\n",
    "\n",
    "print('End training of tVeerCNNf at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_tVeerCNNf+\"model_tVeerCNNf_best.mod\")\n",
    "\n",
    "print('Best epoch of tVeerCNNf: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, \n",
    "           folder_tVeerCNNf+\"model_tVeerCNNf.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_tVeerCNNf+\"model_tVeerCNNf_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_tVeerCNNf, shape_predictions_tVeerCNNf, embeds = model_test_predict(model, test_data_loader_5f, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Embedding encoding\n",
    "emb = []\n",
    "for e in embeds:\n",
    "    #print(e[1][:, :, -1])\n",
    "    emb.append((e[0], e[1][:, :, -1]))\n",
    "embeddings = emb\n",
    "\n",
    "embedding_viz(embeddings, imagefolder+'tVeerCNNf.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_data_tVeerCNNf = {'index': [i for i in range(len(predictions_tVeerCNNf))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_tVeerCNNf,\n",
    "        'shape_prediction': shape_predictions_tVeerCNNf}\n",
    "predictionsDF_tVeerCNNf = pd.DataFrame(pred_data_tVeerCNNf)\n",
    "predictionsDF_tVeerCNNf.to_csv(folder_tVeerCNNf+'predictions_tVeerCNNf.csv')\n",
    "\n",
    "precision_tVeerCNNf, recall_tVeerCNNf, f1_score_tVeerCNNf, support_tVeerCNNf = report_statistics(targets_list, predictions_tVeerCNNf, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_tVeerCNNf = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_tVeerCNNf[predictionsDF_tVeerCNNf['target'] == shapes]\n",
    "    clas_accuracy_tVeerCNNf.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_tVeerCNNf, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dCNN+c\n",
    "\n",
    "Architektur von t'Veer et al. 2019, Tiefe übernommen aus Liu et al. 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dCNNc = folder+'dCNNc/'\n",
    "if not os.path.exists(folder_dCNNc):\n",
    "    os.makedirs(folder_dCNNc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 2\n",
    "hidden_size_1 = 64\n",
    "hidden_size_2 = 1024\n",
    "dense_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCNNc_dims = {2: {'sequence_length': 2,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                3: {'sequence_length': 3,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                4: {'sequence_length': 4,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2},\n",
    "                5: {'sequence_length': 5,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_dCNNc+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "text_file.write('hidden_size_1: ' + str(hidden_size_1) + '\\n')\n",
    "text_file.write('hidden_size_2: ' + str(hidden_size_2) + '\\n')\n",
    "text_file.write('dense_size: ' + str(dense_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "text_file.write('dCNNc_dims:' + '\\n')\n",
    "for key, value in dCNNc_dims[sequence_length].items():\n",
    "    text_file.write(key + ': ' + str(value) + '\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paper Version\n",
    "class dCNNc(nn.Module):\n",
    "    def __init__(self, input_size, dropout, hidden_size_1, hidden_size_2, dense_size, num_classes):\n",
    "        super(dCNNc, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1d_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_size, \n",
    "                      out_channels=hidden_size_1, \n",
    "                      kernel_size = dCNNc_dims[sequence_length]['sequence_length'], \n",
    "                      padding=dCNNc_dims[sequence_length]['conv1d_1_padding']),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        )\n",
    "        \n",
    "        self.conv1d_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = hidden_size_1, out_channels=hidden_size_2, kernel_size = 1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size = dCNNc_dims[sequence_length]['sequence_length'], \n",
    "                         padding = dCNNc_dims[sequence_length]['conv1d_2_padding'])\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense_1 = nn.Linear(hidden_size_2, dense_size)\n",
    "        self.dense_2 = nn.Linear(dense_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1d_1(x)\n",
    "        out = self.conv1d_2(out)\n",
    "        \n",
    "        B, N, C = out.shape\n",
    "        out = out.reshape([B, N*C])\n",
    "        \n",
    "        embedding = out\n",
    "        \n",
    "        out = self.dense_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense_2(out)\n",
    "        return out, embedding\n",
    "        \n",
    "        \n",
    "model = dCNNc(hp['input_size'], hp['rec_dropout'], hidden_size_1, hidden_size_2, dense_size, hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], hp['input_size'], sequence_length), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Train the dCNNc model\n",
    "\n",
    "print('Start training of dCNNc at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_dCNNc = []\n",
    "test_loss_dCNNc = []\n",
    "test_accuracy_dCNNc = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_c = data.DataLoader(train_dataset_c, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_c:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs.float()\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_dCNNc.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_c:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_.float()\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_dCNNc.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_dCNNc.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1,\n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_dCNNc+\"model_dCNNc_best.mod\")\n",
    "    \n",
    "print('End training of dCNNc at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_dCNNc+\"model_dCNNc_best.mod\")\n",
    "\n",
    "print('Best epoch of dCNNc: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, folder_dCNNc+\"model_dCNNc.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_dCNNc+\"model_dCNNc_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_dCNNc, shape_predictions_dCNNc, embeddings = model_test_predict(model, test_data_loader_c, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'dCNNc.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "targets_list, targets_shape, predictions_dCNNc, shape_predictions_dCNNc, embeds = model_test_predict(model, test_data_loader_c, shape_dict_reverse)\n",
    "\n",
    "pred_data_dCNNc = {'index': [i for i in range(len(predictions_dCNNc))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_dCNNc,\n",
    "        'shape_prediction': shape_predictions_dCNNc}\n",
    "predictionsDF_dCNNc = pd.DataFrame(pred_data_dCNNc)\n",
    "predictionsDF_dCNNc.to_csv(folder_dCNNc+'predictions_dCNNc.csv')\n",
    "\n",
    "precision_dCNNc, recall_dCNNc, f1_score_dCNNc, support_dCNNc = report_statistics(targets_list, predictions_dCNNc, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_dCNNc = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_dCNNc[predictionsDF_dCNNc['target'] == shapes]\n",
    "    clas_accuracy_dCNNc.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_dCNNc, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dCNN+s\n",
    "\n",
    "Architektur von t'Veer et al. 2019, Tiefe übernommen aus Liu et al. 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dCNNs = folder+'dCNNs/'\n",
    "if not os.path.exists(folder_dCNNs):\n",
    "    os.makedirs(folder_dCNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "hidden_size_1 = 64\n",
    "hidden_size_2 = 1024\n",
    "dense_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCNNs_dims = {2: {'sequence_length': 2,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                3: {'sequence_length': 3,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                4: {'sequence_length': 4,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2},\n",
    "                5: {'sequence_length': 5,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_dCNNs+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "text_file.write('hidden_size_1: ' + str(hidden_size_1) + '\\n')\n",
    "text_file.write('hidden_size_2: ' + str(hidden_size_2) + '\\n')\n",
    "text_file.write('dense_size: ' + str(dense_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "text_file.write('dCNNs_dims:' + '\\n')\n",
    "for key, value in dCNNs_dims[sequence_length].items():\n",
    "    text_file.write(key + ': ' + str(value) + '\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paper Version\n",
    "class dCNNs(nn.Module):\n",
    "    def __init__(self, input_size, dropout, hidden_size_1, hidden_size_2, dense_size, num_classes):\n",
    "        super(dCNNs, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1d_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_size, \n",
    "                      out_channels=hidden_size_1, \n",
    "                      kernel_size = dCNNs_dims[sequence_length]['sequence_length'], \n",
    "                      padding=dCNNs_dims[sequence_length]['conv1d_1_padding']),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        )\n",
    "        \n",
    "        self.conv1d_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = hidden_size_1, out_channels=hidden_size_2, kernel_size = 1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size = dCNNs_dims[sequence_length]['sequence_length'], \n",
    "                         padding = dCNNs_dims[sequence_length]['conv1d_2_padding'])\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense_1 = nn.Linear(hidden_size_2, dense_size)\n",
    "        self.dense_2 = nn.Linear(dense_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1d_1(x)\n",
    "        out = self.conv1d_2(out)\n",
    "        \n",
    "        B, N, C = out.shape\n",
    "        out = out.reshape([B, N*C])\n",
    "        \n",
    "        embedding = out\n",
    "        \n",
    "        out = self.dense_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense_2(out)\n",
    "        return out, embedding\n",
    "        \n",
    "        \n",
    "model = dCNNs(hp['input_size'], hp['rec_dropout'], hidden_size_1, hidden_size_2, dense_size, hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], hp['input_size'], sequence_length), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the dCNNs model\n",
    "\n",
    "print('Start training of dCNNs at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_dCNNs = []\n",
    "test_loss_dCNNs = []\n",
    "test_accuracy_dCNNs = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_s = data.DataLoader(train_dataset_s, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_s:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs.float()\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_dCNNs.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_s:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_.float()\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_dCNNs.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_dCNNs.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1, \n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_dCNNs+\"model_dCNNs_best.mod\")\n",
    "    \n",
    "print('End training of dCNNs at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_dCNNs+\"model_dCNNs_best.mod\")\n",
    "\n",
    "print('Best epoch of dCNNs: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, folder_dCNNs+\"model_dCNNs.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_dCNNs+\"model_dCNNs_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_dCNNs, shape_predictions_dCNNs, embeddings = model_test_predict(model, test_data_loader_s, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'dCNNs.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "targets_list, targets_shape, predictions_dCNNs, shape_predictions_dCNNs, embeds = model_test_predict(model, test_data_loader_s, shape_dict_reverse)\n",
    "\n",
    "pred_data_dCNNs = {'index': [i for i in range(len(predictions_dCNNs))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_dCNNs,\n",
    "        'shape_prediction': shape_predictions_dCNNs}\n",
    "predictionsDF_dCNNs = pd.DataFrame(pred_data_dCNNs)\n",
    "predictionsDF_dCNNs.to_csv(folder_dCNNs+'predictions_dCNNs.csv')\n",
    "\n",
    "precision_dCNNs, recall_dCNNs, f1_score_dCNNs, support_dCNNs = report_statistics(targets_list, predictions_dCNNs, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_dCNNs = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_dCNNs[predictionsDF_dCNNs['target'] == shapes]\n",
    "    clas_accuracy_dCNNs.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_dCNNs, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dCNN+f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dCNNf = folder+'dCNNf/'\n",
    "if not os.path.exists(folder_dCNNf):\n",
    "    os.makedirs(folder_dCNNf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length_20 = data_inputs_5f.shape[2]\n",
    "hidden_size_1 = 64\n",
    "hidden_size_2 = 1024\n",
    "dense_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCNNf_dims = {2: {'sequence_length': 2,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                3: {'sequence_length': 3,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 1},\n",
    "                4: {'sequence_length': 4,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2},\n",
    "                5: {'sequence_length': 5,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'conv1d_2_padding': 2},\n",
    "                 20: {'sequence_length': 20,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'sequence_length_2': 4,\n",
    "                    'conv1d_2_padding': 2},\n",
    "             22: {'sequence_length': 22,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'sequence_length_2': 4,\n",
    "                    'conv1d_2_padding': 2},\n",
    "              24: {'sequence_length': 24,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'sequence_length_2': 4,\n",
    "                    'conv1d_2_padding': 2},\n",
    "             25: {'sequence_length': 25,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'sequence_length_2': 4,\n",
    "                    'conv1d_2_padding': 2},\n",
    "             26: {'sequence_length': 26,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'sequence_length_2': 4,\n",
    "                    'conv1d_2_padding': 2},\n",
    "             29: {'sequence_length': 29,\n",
    "                    'conv1d_1_padding': 2,\n",
    "                    'sequence_length_2': 4,\n",
    "                    'conv1d_2_padding': 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_dCNNf+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length_20) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "text_file.write('hidden_size_1: ' + str(hidden_size_1) + '\\n')\n",
    "text_file.write('hidden_size_2: ' + str(hidden_size_2) + '\\n')\n",
    "text_file.write('dense_size: ' + str(dense_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "text_file.write('dCNNf_dims:' + '\\n')\n",
    "for key, value in dCNNf_dims[sequence_length].items():\n",
    "    text_file.write(key + ': ' + str(value) + '\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paper Version\n",
    "class dCNNf(nn.Module):\n",
    "    def __init__(self, input_size, dropout, hidden_size_1, hidden_size_2, dense_size, num_classes):\n",
    "        super(dCNNf, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1d_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_size, \n",
    "                      out_channels=hidden_size_1, \n",
    "                      kernel_size = dCNNf_dims[sequence_length_20]['sequence_length'], \n",
    "                      padding=dCNNf_dims[sequence_length_20]['conv1d_1_padding']),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3)#, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.conv1d_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = hidden_size_1, out_channels=hidden_size_2, kernel_size = 1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size = dCNNf_dims[sequence_length_20]['sequence_length_2'], \n",
    "                         padding = dCNNf_dims[sequence_length_20]['conv1d_2_padding'])\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense_1 = nn.Linear(hidden_size_2, dense_size)\n",
    "        self.dense_2 = nn.Linear(dense_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1d_1(x)\n",
    "        out = self.conv1d_2(out)\n",
    "        \n",
    "        B, N, C = out.shape\n",
    "        out = out.reshape([B,N*C])\n",
    "        \n",
    "        embedding = out\n",
    "        \n",
    "        out = self.dense_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense_2(out)\n",
    "        return out, embedding\n",
    "        \n",
    "        \n",
    "model = dCNNf(hp['input_size'], hp['rec_dropout'], hidden_size_1, hidden_size_2, dense_size, hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], hp['input_size'], sequence_length_20), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the dCNNf model\n",
    "\n",
    "print('Start training of dCNNf at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_dCNNf = []\n",
    "test_loss_dCNNf = []\n",
    "test_accuracy_dCNNf = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_5f = data.DataLoader(train_dataset_5f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_5f:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs.float()\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_dCNNf.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_5f:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_.float()\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_dCNNf.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_dCNNf.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1, \n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_dCNNf+\"model_dCNNf_best.mod\")\n",
    "    \n",
    "print('End training of dCNNf at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_dCNNf+\"model_dCNNf_best.mod\")\n",
    "\n",
    "print('Best epoch of dCNNf: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, folder_dCNNf+\"model_dCNNf.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_dCNNf+\"model_dCNNf_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_dCNNf, shape_predictions_dCNNf, embeddings = model_test_predict(model, test_data_loader_5f, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'dCNNf.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_data_dCNNf = {'index': [i for i in range(len(predictions_dCNNf))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_dCNNf,\n",
    "        'shape_prediction': shape_predictions_dCNNf}\n",
    "predictionsDF_dCNNf = pd.DataFrame(pred_data_dCNNf)\n",
    "predictionsDF_dCNNf.to_csv(folder_dCNNf+'predictions_dCNNf.csv')\n",
    "\n",
    "\n",
    "precision_dCNNf, recall_dCNNf, f1_score_dCNNf, support_dCNNf = report_statistics(targets_list, predictions_dCNNf, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_dCNNf = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_dCNNf[predictionsDF_dCNNf['target'] == shapes]\n",
    "    clas_accuracy_dCNNf.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_dCNNf, test_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriangleCNN\n",
    "\n",
    "Source: Liu et al. 2021: TriangleConv: A Deep Point Convolutional Network for Recognizing Building Shapes in Map Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_triangleCNN = folder+'triangleCNN/'\n",
    "if not os.path.exists(folder_triangleCNN):\n",
    "    os.makedirs(folder_triangleCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_triangleCNN+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def conv_bn_block(input, output, kernel_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(input, output, kernel_size),\n",
    "        nn.BatchNorm1d(output),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def fc_bn_block(input, output):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input, output),\n",
    "        nn.BatchNorm1d(output),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class TriangleConv(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(TriangleConv, self).__init__()\n",
    "        self.layers = layers\n",
    "        mlp_layers = OrderedDict()\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            if i == 0:\n",
    "                mlp_layers['conv_bn_block_{}'.format(i + 1)] = conv_bn_block(4 * self.layers[i], self.layers[i + 1], 1)\n",
    "            else:\n",
    "                mlp_layers['conv_bn_block_{}'.format(i + 1)] = conv_bn_block(self.layers[i], self.layers[i + 1], 1)\n",
    "        self.mlp = nn.Sequential(mlp_layers)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, N, F = X.shape\n",
    "        k_indexes = []\n",
    "        for i in range(N):\n",
    "            if i == 0:\n",
    "                k_indexes.append([N - 1, i + 1])\n",
    "            elif i == N-1:\n",
    "                k_indexes.append([i - 1, 0])\n",
    "            else:\n",
    "                k_indexes.append([i - 1, i+1])\n",
    "        k_indexes_tensor = torch.Tensor(k_indexes)\n",
    "        k_indexes_tensor = k_indexes_tensor.long()\n",
    "        x1 = torch.zeros(B, N, 2, F).to(device)\n",
    "        for idx, x in enumerate(X):\n",
    "            x1[idx] = x[k_indexes_tensor]\n",
    "        x2 = X.reshape([B, N, 1, F]).float()\n",
    "        x2 = x2.expand(B, N, 2, F)\n",
    "        x2 = x2-x1\n",
    "        x3 = x2[:, :, 0:1, :]\n",
    "        x4 = x2[:, :, 1:2, :]\n",
    "        x4 = x3-x4\n",
    "        x5 = X.reshape([B, N, 1, F]).float()\n",
    "        x2 = x2.reshape([B, N, 1, 2*F])\n",
    "        x_triangle = torch.cat([x5, x2, x4], dim=3)\n",
    "        x_triangle=torch.squeeze(x_triangle)\n",
    "        x_triangle = x_triangle.permute(0, 2, 1)\n",
    "        x_triangle = torch.tensor(x_triangle,dtype=torch.float32).to(device)\n",
    "        out = self.mlp(x_triangle)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class triangleCNN(nn.Module):\n",
    "    def __init__(self, dropout, num_classes):\n",
    "        super(triangleCNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.triangleconv_1 = TriangleConv(layers=[2, 64, 64, 64])\n",
    "        self.triangleconv_2 = TriangleConv(layers=[64, 512, 1024])\n",
    "        self.fc_block_4 = fc_bn_block(1024, 512)\n",
    "        self.drop_4 = nn.Dropout(dropout)\n",
    "        self.fc_block_5 = fc_bn_block(512, 256)\n",
    "        self.drop_5 = nn.Dropout(dropout)\n",
    "        self.fc_6 = nn.Linear(256, self.num_classes)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        assert C == 2, 'dimension of x does not match'\n",
    "        x = self.triangleconv_1(x)\n",
    "        x = self.triangleconv_2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = nn.MaxPool1d(N)(x)\n",
    "        x = x.reshape([B, 1024])\n",
    "        embedding = x\n",
    "        x = self.fc_block_4(x)\n",
    "        x = self.drop_4(x)\n",
    "        x = self.fc_block_5(x)\n",
    "        x = self.drop_5(x)\n",
    "        x = self.fc_6(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        return x, embedding\n",
    "\n",
    "model = triangleCNN(hp['dropout'], hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], hp['input_size'], sequence_length), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the triangleCNN model\n",
    "\n",
    "print('Start training of TriangleCNN at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_triangleCNN = []\n",
    "test_loss_triangleCNN = []\n",
    "test_accuracy_triangleCNN = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_c = data.DataLoader(train_dataset_c, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_c:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs.float()\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_triangleCNN.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_c:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_.float()\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_triangleCNN.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_triangleCNN.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1, \n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_triangleCNN+\"model_triangleCNN_best.mod\")\n",
    "    \n",
    "    \n",
    "print('End training of TriangleCNN at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_triangleCNN+\"model_triangleCNN_best.mod\")\n",
    "\n",
    "print('Best epoch of triangleCNN: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, folder_triangleCNN+\"model_triangleCNN.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_triangleCNN+\"model_triangleCNN_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_triangleCNN, shape_predictions_triangleCNN, embeddings = model_test_predict(model, test_data_loader_c, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'triangleCNN.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "targets_list, targets_shape, predictions_triangleCNN, shape_predictions_triangleCNN, embeds = model_test_predict(model, test_data_loader_c, shape_dict_reverse)\n",
    "\n",
    "pred_data_triangleCNN = {'index': [i for i in range(len(predictions_triangleCNN))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_triangleCNN,\n",
    "        'shape_prediction': shape_predictions_triangleCNN}\n",
    "predictionsDF_triangleCNN = pd.DataFrame(pred_data_triangleCNN)\n",
    "predictionsDF_triangleCNN.to_csv(folder_triangleCNN+'predictions_triangleCNN.csv')\n",
    "\n",
    "precision_triangleCNN, recall_triangleCNN, f1_score_triangleCNN, support_triangleCNN = report_statistics(targets_list, predictions_triangleCNN, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_triangleCNN = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_triangleCNN[predictionsDF_triangleCNN['target'] == shapes]\n",
    "    clas_accuracy_triangleCNN.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_triangleCNN, test_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tVeerRNN+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tVeerRNNc = folder+'tVeerRNNc/'\n",
    "if not os.path.exists(folder_tVeerRNNc):\n",
    "    os.makedirs(folder_tVeerRNNc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 2\n",
    "hidden_size = 256\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tVeerRNNc_dims = {2: {'maxpool_kernel': 2},\n",
    "                  3: {'maxpool_kernel': 3},\n",
    "                  4: {'maxpool_kernel': 4},\n",
    "                  5: {'maxpool_kernel': 2},\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_tVeerRNNc+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "text_file.write('rec_dropout: ' + str(hp['rec_dropout']) + '\\n')\n",
    "text_file.write('hidden_size: ' + str(hidden_size) + '\\n')\n",
    "text_file.write('num_layers: ' + str(num_layers) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class tVeerRNNc(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, rec_dropout, dropout, num_classes):\n",
    "        super(tVeerRNNc, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=rec_dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(device)\n",
    "        #print(x.shape[1])\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "\n",
    "        B, N, C = out.shape\n",
    "            #print(out.shape)\n",
    "            # Decode the hidden state of the last time step\n",
    "        out = nn.MaxPool1d(N)(out)\n",
    "            #print(out.shape)\n",
    "\n",
    "        out = out.reshape([B, C])\n",
    "        \n",
    "        embedding = out\n",
    "\n",
    "        out = self.dense(out)\n",
    "        \n",
    "            \n",
    "        return out, embedding\n",
    "\n",
    "model = tVeerRNNc(hp['input_size'], hidden_size, num_layers, hp['rec_dropout'], hp['dropout'], hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], sequence_length, hp['input_size']), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the tVeerRNNc model\n",
    "\n",
    "print('Start training of tVeerRNNc at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_tVeerRNNc = []\n",
    "test_loss_tVeerRNNc = []\n",
    "test_accuracy_tVeerRNNc = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_c = data.DataLoader(train_dataset_c, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_c:\n",
    "        data_inputs, data_labels = batch\n",
    "\n",
    "        outputs, _ = model(torch.transpose(data_inputs, 1, 2).float())\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_tVeerRNNc.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_c:  \n",
    "        input_, targets = batch\n",
    "\n",
    "        outputs, _ = model(torch.transpose(input_, 1, 2).float())\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_tVeerRNNc.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_tVeerRNNc.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1, \n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_tVeerRNNc+\"model_tVeerRNNc_best.mod\")    \n",
    "    \n",
    "print('End training of tVeerRNNc at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_tVeerRNNc+\"model_tVeerRNNc_best.mod\")\n",
    "\n",
    "print('Best epoch of tVeerRNNc: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, folder_tVeerRNNc+\"model_tVeerRNNc.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_tVeerRNNc+\"model_tVeerRNNc_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_tVeerRNNc, shape_predictions_tVeerRNNc, embeddings = model_test_predict(model, test_data_loader_c, shape_dict_reverse, transpose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'tVeerRNNc.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_data_tVeerRNNc = {'index': [i for i in range(len(predictions_tVeerRNNc))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_tVeerRNNc,\n",
    "        'shape_prediction': shape_predictions_tVeerRNNc}\n",
    "predictionsDF_tVeerRNNc = pd.DataFrame(pred_data_tVeerRNNc)\n",
    "predictionsDF_tVeerRNNc.to_csv(folder_tVeerRNNc+'predictions_tVeerRNNc.csv')\n",
    "\n",
    "\n",
    "precision_tVeerRNNc, recall_tVeerRNNc, f1_score_tVeerRNNc, support_tVeerRNNc = report_statistics(targets_list, predictions_tVeerRNNc, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_tVeerRNNc = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_tVeerRNNc[predictionsDF_tVeerRNNc['target'] == shapes]\n",
    "    clas_accuracy_tVeerRNNc.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_tVeerRNNc, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tVeerRNN+s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tVeerRNNs = folder+'tVeerRNNs/'\n",
    "if not os.path.exists(folder_tVeerRNNs):\n",
    "    os.makedirs(folder_tVeerRNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "hidden_size = 256\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tVeerRNNs_dims = {2: {'maxpool_kernel': 2},\n",
    "                  3: {'maxpool_kernel': 3},\n",
    "                  4: {'maxpool_kernel': 4},\n",
    "                  5: {'maxpool_kernel': 2},\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_tVeerRNNs+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "text_file.write('rec_dropout: ' + str(hp['rec_dropout']) + '\\n')\n",
    "text_file.write('hidden_size: ' + str(hidden_size) + '\\n')\n",
    "text_file.write('num_layers: ' + str(num_layers) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class tVeerRNNs(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, rec_dropout, dropout, num_classes):\n",
    "        super(tVeerRNNs, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=rec_dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(device)\n",
    "        #print(x.shape[1])\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "            \n",
    "        embedding = out[:, -1, :]\n",
    "        out = self.dense(out[:, -1, :])\n",
    "            \n",
    "        return out, embedding\n",
    "\n",
    "model = tVeerRNNs(hp['input_size'], hidden_size, num_layers, hp['rec_dropout'], hp['dropout'], hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], sequence_length, hp['input_size']), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the tVeerRNNs model\n",
    "\n",
    "print('Start training of tVeerRNNs at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_tVeerRNNs = []\n",
    "test_loss_tVeerRNNs = []\n",
    "test_accuracy_tVeerRNNs = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_s = data.DataLoader(train_dataset_s, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_s:\n",
    "        data_inputs, data_labels = batch\n",
    "\n",
    "        outputs, _ = model(torch.transpose(data_inputs, 1, 2).float())\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_tVeerRNNs.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_s:  \n",
    "        input_, targets = batch\n",
    "\n",
    "        outputs, _ = model(torch.transpose(input_, 1, 2).float())\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_tVeerRNNs.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_tVeerRNNs.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1,\n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_tVeerRNNs+\"model_tVeerRNNs_best.mod\")    \n",
    "    \n",
    "print('End training of tVeerRNNs at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_tVeerRNNs+\"model_tVeerRNNs_best.mod\")\n",
    "\n",
    "print('Best epoch of tVeerRNNs: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, folder_tVeerRNNs+\"model_tVeerRNNs.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_tVeerRNNs+\"model_tVeerRNNs_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_tVeerRNNs, shape_predictions_tVeerRNNs, embeddings = model_test_predict(model, test_data_loader_s, shape_dict_reverse, transpose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'tVeerRNNs.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_data_tVeerRNNs = {'index': [i for i in range(len(predictions_tVeerRNNs))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_tVeerRNNs,\n",
    "        'shape_prediction': shape_predictions_tVeerRNNs}\n",
    "predictionsDF_tVeerRNNs = pd.DataFrame(pred_data_tVeerRNNs)\n",
    "predictionsDF_tVeerRNNs.to_csv(folder_tVeerRNNs+'predictions_tVeerRNNs.csv')\n",
    "\n",
    "precision_tVeerRNNs, recall_tVeerRNNs, f1_score_tVeerRNNs, support_tVeerRNNs = report_statistics(targets_list, predictions_tVeerRNNs, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_tVeerRNNs = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_tVeerRNNs[predictionsDF_tVeerRNNs['target'] == shapes]\n",
    "    clas_accuracy_tVeerRNNs.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_tVeerRNNs, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tVeerRNN+f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tVeerRNNf = folder+'tVeerRNNf/'\n",
    "if not os.path.exists(folder_tVeerRNNf):\n",
    "    os.makedirs(folder_tVeerRNNf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = data_inputs_5f.shape[2]\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "print(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_tVeerRNNf+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('sequence_length: ' + str(sequence_length) + '\\n')\n",
    "text_file.write('input_size: ' + str(hp['input_size']) + '\\n')\n",
    "text_file.write('dropout: ' + str(hp['dropout']) + '\\n')\n",
    "text_file.write('rec_dropout: ' + str(hp['rec_dropout']) + '\\n')\n",
    "text_file.write('hidden_size: ' + str(hidden_size) + '\\n')\n",
    "text_file.write('num_layers: ' + str(num_layers) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class tVeerRNNf(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, rec_dropout, dropout, num_classes):\n",
    "        super(tVeerRNNf, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=rec_dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Linear(hidden_size*2*sequence_length, num_classes)  # 2 for bidirection\n",
    "        \n",
    "        self.dense_1 = nn.Linear(hidden_size*2*sequence_length, hidden_size*2)  # 2 for bidirection\n",
    "        self.dense_2 = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        embedding = out[:, -1, :]\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        B, N, C = out.shape\n",
    "        # Decode the hidden state of the last time step\n",
    "        #out = nn.MaxPool1d(N)(out)\n",
    "        out = out.reshape([B,N*C])\n",
    "        \n",
    "        embedding = out\n",
    "        \n",
    "        #out = self.dense(out)\n",
    "        \n",
    "        out = self.dense_1(out)\n",
    "        out = self.dense_2(out)\n",
    "                                                              \n",
    "        # Decode the hidden state of the last time step\n",
    "        #out = self.dense(out[:, -1, :])\n",
    "        return out, embedding\n",
    "\n",
    "model = tVeerRNNf(hp['input_size'], hidden_size, num_layers, hp['rec_dropout'], hp['dropout'], hp['num_classes']).to(device)\n",
    "summary(model, input_size=(hp['batch_size'], sequence_length, hp['input_size']), verbose = 0, col_names = (\"input_size\", \"output_size\", \"num_params\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the tVeerRNNf model\n",
    "\n",
    "print('Start training of tVeerRNNf at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_tVeerRNNf = []\n",
    "test_loss_tVeerRNNf = []\n",
    "test_accuracy_tVeerRNNf = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_5f = data.DataLoader(train_dataset_5f, batch_size=hp['batch_size'], shuffle=hp['shuffled_train'], drop_last=True)\n",
    "    \n",
    "    for batch in train_data_loader_5f:\n",
    "        data_inputs, data_labels = batch\n",
    "\n",
    "        outputs, _ = model(torch.transpose(data_inputs, 1, 2).float())\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_tVeerRNNf.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_5f:  \n",
    "        input_, targets = batch\n",
    "\n",
    "        outputs, _ = model(torch.transpose(input_, 1, 2).float())\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_tVeerRNNf.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_tVeerRNNf.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1,\n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_tVeerRNNf+\"model_tVeerRNNf_best.mod\")    \n",
    "    \n",
    "print('End training of tVeerRNNf at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_tVeerRNNf+\"model_tVeerRNNf_best.mod\")\n",
    "\n",
    "print('Best epoch of tVeerRNNf: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, folder_tVeerRNNf+\"model_tVeerRNNf.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_tVeerRNNf+\"model_tVeerRNNf_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_tVeerRNNf, shape_predictions_tVeerRNNf, embeddings = model_test_predict(model, test_data_loader_5f, shape_dict_reverse, transpose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'tVeerRNNf.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"colors.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_data_tVeerRNNf = {'index': [i for i in range(len(predictions_tVeerRNNf))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_tVeerRNNf,\n",
    "        'shape_prediction': shape_predictions_tVeerRNNf}\n",
    "predictionsDF_tVeerRNNf = pd.DataFrame(pred_data_tVeerRNNf)\n",
    "predictionsDF_tVeerRNNf.to_csv(folder_tVeerRNNf+'predictions_tVeerRNNf.csv')\n",
    "\n",
    "precision_tVeerRNNf, recall_tVeerRNNf, f1_score_tVeerRNNf, support_tVeerRNNf = report_statistics(targets_list, predictions_tVeerRNNf, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_tVeerRNNf = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_tVeerRNNf[predictionsDF_tVeerRNNf['target'] == shapes]\n",
    "    clas_accuracy_tVeerRNNf.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_tVeerRNNf, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCNN+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_GCNNc = folder+'GCNNc/'\n",
    "if not os.path.exists(folder_GCNNc):\n",
    "    os.makedirs(folder_GCNNc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_G = data_inputs_graph_c.ndata['x'].shape[1]\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_GCNNc+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('input_size_G: ' + str(input_size_G) + '\\n')\n",
    "text_file.write('hidden_size: ' + str(hidden_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Version\n",
    "class GCNNc(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(GCNNc, self).__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, 256)\n",
    "        self.dense = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "\n",
    "        h = g.ndata['x'].float()\n",
    "        h = F.leaky_relu(self.conv1(g, h))\n",
    "        h = F.leaky_relu(self.conv2(g, h))\n",
    "\n",
    "        g.ndata['x'] = h\n",
    "        hg = dgl.mean_nodes(g, 'x')\n",
    "        \n",
    "        embeddings = hg\n",
    "        \n",
    "        hg = self.dense(hg)\n",
    "        return hg, embeddings\n",
    "        \n",
    "model = GCNNc(input_size_G, hidden_size, hp['num_classes']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train the GCNNc model\n",
    "\n",
    "print('Start training of GCNNc at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_GCNNc = []\n",
    "test_loss_GCNNc = []\n",
    "test_accuracy_GCNNc = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_graph_c = data.DataLoader(train_dataset_graph_c, batch_size=hp['batch_size'], \n",
    "                                                  shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "    \n",
    "    for batch in train_data_loader_graph_c:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_GCNNc.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_graph_c:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_GCNNc.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_GCNNc.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1, \n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_GCNNc+\"model_GCNNc_best.mod\")\n",
    "    \n",
    "print('End training of GCNNc at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_GCNNc+\"model_GCNNc_best.mod\")\n",
    "\n",
    "print('Best epoch of GCNNc: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n",
    "           folder_GCNNc+\"model_GCNNc.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_GCNNc+\"model_GCNNc_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_GCNNc, shape_predictions_GCNNc, embeddings = model_test_predict(model, test_data_loader_graph_c, shape_dict_reverse, graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'GCNNc.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_data_GCNNc = {'index': [i for i in range(len(predictions_GCNNc))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_GCNNc,\n",
    "        'shape_prediction': shape_predictions_GCNNc}\n",
    "predictionsDF_GCNNc = pd.DataFrame(pred_data_GCNNc)\n",
    "predictionsDF_GCNNc.to_csv(folder_GCNNc+'predictions_GCNNc.csv')\n",
    "\n",
    "precision_GCNNc, recall_GCNNc, f1_score_GCNNc, support_GCNNc = report_statistics(targets_list, predictions_GCNNc, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_GCNNc = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_GCNNc[predictionsDF_GCNNc['target'] == shapes]\n",
    "    clas_accuracy_GCNNc.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_GCNNc, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCNN+s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_GCNNs = folder+'GCNNs/'\n",
    "if not os.path.exists(folder_GCNNs):\n",
    "    os.makedirs(folder_GCNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_s = data_inputs_graph_s.ndata['x'].shape[1]\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_GCNNs+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('input_size_G: ' + str(input_size_G) + '\\n')\n",
    "text_file.write('hidden_size: ' + str(hidden_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCNNs(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(GCNNs, self).__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, 256)\n",
    "        self.dense = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "\n",
    "        h = g.ndata['x'].float()\n",
    "        h = F.leaky_relu(self.conv1(g, h))\n",
    "        h = F.leaky_relu(self.conv2(g, h))\n",
    "\n",
    "        g.ndata['x'] = h\n",
    "        hg = dgl.mean_nodes(g, 'x')\n",
    "        \n",
    "        embeddings = hg\n",
    "        \n",
    "        hg = self.dense(hg)\n",
    "        return hg, embeddings\n",
    "        \n",
    "model = GCNNs(input_size_s, hidden_size, hp['num_classes']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train the GCNNs model\n",
    "\n",
    "print('Start training of GCNNs at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_GCNNs = []\n",
    "test_loss_GCNNs = []\n",
    "test_accuracy_GCNNs = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_graph_s = data.DataLoader(train_dataset_graph_s, batch_size=hp['batch_size'], \n",
    "                                                  shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "    \n",
    "    for batch in train_data_loader_graph_s:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_GCNNs.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_graph_s:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_GCNNs.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_GCNNs.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1, \n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_GCNNs+\"model_GCNNs_best.mod\")\n",
    "    \n",
    "print('End training of GCNNs at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_GCNNs+\"model_GCNNs_best.mod\")\n",
    "\n",
    "print('Best epoch of GCNNs: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n",
    "           folder_GCNNs+\"model_GCNNs.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_GCNNs+\"model_GCNNs_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_GCNNs, shape_predictions_GCNNs, embeddings = model_test_predict(model, test_data_loader_graph_s, shape_dict_reverse, graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'GCNNs.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_data_GCNNs = {'index': [i for i in range(len(predictions_GCNNs))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_GCNNs,\n",
    "        'shape_prediction': shape_predictions_GCNNs}\n",
    "predictionsDF_GCNNs = pd.DataFrame(pred_data_GCNNs)\n",
    "predictionsDF_GCNNs.to_csv(folder_GCNNs+'predictions_GCNNs.csv')\n",
    "\n",
    "precision_GCNNs, recall_GCNNs, f1_score_GCNNs, support_GCNNs = report_statistics(targets_list, predictions_GCNNs, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_GCNNs = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_GCNNs[predictionsDF_GCNNs['target'] == shapes]\n",
    "    clas_accuracy_GCNNs.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_GCNNs, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YanGCNN+f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_YanGCNNf = folder+'YanGCNNf/'\n",
    "if not os.path.exists(folder_YanGCNNf):\n",
    "    os.makedirs(folder_YanGCNNf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_Gf = data_inputs_graph_f.ndata['x'].shape[1]\n",
    "print(input_size_Gf)\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_YanGCNNf+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('input_size_G: ' + str(input_size_Gf) + '\\n')\n",
    "text_file.write('hidden_size: ' + str(hidden_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Version\n",
    "class YanGCNNf(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(YanGCNNf, self).__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, 256)\n",
    "        self.dense = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "\n",
    "        h = g.ndata['x'].float()\n",
    "        h = F.leaky_relu(self.conv1(g, h))\n",
    "        h = F.leaky_relu(self.conv2(g, h))\n",
    "\n",
    "        g.ndata['x'] = h\n",
    "        hg = dgl.mean_nodes(g, 'x')\n",
    "        \n",
    "        embeddings = hg\n",
    "        \n",
    "        hg = self.dense(hg)\n",
    "        return hg, embeddings\n",
    "        \n",
    "model = YanGCNNf(input_size_Gf, hidden_size, hp['num_classes']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train the YanGCNNf model\n",
    "\n",
    "print('Start training of YanGCNNf at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_YanGCNNf = []\n",
    "test_loss_YanGCNNf = []\n",
    "test_accuracy_YanGCNNf = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_graph_f = data.DataLoader(train_dataset_graph_f, batch_size=hp['batch_size'], \n",
    "                                                    shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "    \n",
    "    for batch in train_data_loader_graph_f:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_YanGCNNf.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_graph_f:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_YanGCNNf.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_YanGCNNf.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1, \n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_YanGCNNf+\"model_YanGCNNf_best.mod\")\n",
    "    \n",
    "print('End training of YanGCNNf at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_YanGCNNf+\"model_YanGCNNf_best.mod\")\n",
    "\n",
    "print('Best epoch of YanGCNNf: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n",
    "           folder_YanGCNNf+\"model_YanGCNNf.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_YanGCNNf+\"model_YanGCNNf_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_YanGCNNf, shape_predictions_YanGCNNf, embeddings = model_test_predict(model, test_data_loader_graph_f, shape_dict_reverse, graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'YanGCNNf.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"colors.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_data_YanGCNNf = {'index': [i for i in range(len(predictions_YanGCNNf))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_YanGCNNf,\n",
    "        'shape_prediction': shape_predictions_YanGCNNf}\n",
    "predictionsDF_YanGCNNf = pd.DataFrame(pred_data_YanGCNNf)\n",
    "predictionsDF_YanGCNNf.to_csv(folder_YanGCNNf+'predictions_YanGCNNf.csv')\n",
    "\n",
    "precision_YanGCNNf, recall_YanGCNNf, f1_score_YanGCNNf, support_YanGCNNf = report_statistics(targets_list, predictions_YanGCNNf, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_YanGCNNf = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_YanGCNNf[predictionsDF_YanGCNNf['target'] == shapes]\n",
    "    clas_accuracy_YanGCNNf.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_YanGCNNf, test_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCNN+f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_GCNNf = folder+'GCNNf/'\n",
    "if not os.path.exists(folder_GCNNf):\n",
    "    os.makedirs(folder_GCNNf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_5f = data_inputs_graph_5f.ndata['x'].shape[1]\n",
    "print(input_size_5f)\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder_GCNNf+ 'params.txt', 'w')\n",
    "text_file.write('hyperparameters:' + '\\n')\n",
    "text_file.write('input_size_G: ' + str(input_size_5f) + '\\n')\n",
    "text_file.write('hidden_size: ' + str(hidden_size) + '\\n')\n",
    "text_file.write('num_classes: ' + str(hp['num_classes']) + '\\n')\n",
    "text_file.write('\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Version\n",
    "class GCNNf(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(GCNNf, self).__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, 256)\n",
    "        self.dense = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "\n",
    "        h = g.ndata['x'].float()\n",
    "        h = F.leaky_relu(self.conv1(g, h))\n",
    "        h = F.leaky_relu(self.conv2(g, h))\n",
    "\n",
    "        g.ndata['x'] = h\n",
    "        hg = dgl.mean_nodes(g, 'x')\n",
    "        \n",
    "        embeddings = hg\n",
    "        \n",
    "        hg = self.dense(hg)\n",
    "        return hg, embeddings\n",
    "        \n",
    "model = GCNNf(input_size_5f, hidden_size, hp['num_classes']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train the GCNNf model\n",
    "\n",
    "print('Start training of GCNNf at ' + str(datetime.now()))\n",
    "\n",
    "training_loss_GCNNf = []\n",
    "test_loss_GCNNf = []\n",
    "test_accuracy_GCNNf = []\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(hp['num_epochs']):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    if hp['shuffled_train']:\n",
    "        train_data_loader_graph_5f = data.DataLoader(train_dataset_graph_5f, batch_size=hp['batch_size'], \n",
    "                                                    shuffle=hp['shuffled_train'], drop_last=True, collate_fn=collate)\n",
    "    \n",
    "    for batch in train_data_loader_graph_5f:\n",
    "        data_inputs, data_labels = batch\n",
    "        data_inputs = data_inputs\n",
    "\n",
    "        outputs, _ = model(data_inputs)\n",
    "\n",
    "        loss = criterion(outputs, data_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    training_loss_GCNNf.append(loss.item())\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_data_loader_graph_5f:  \n",
    "        input_, targets = batch\n",
    "        input_ = input_\n",
    "\n",
    "        outputs, _ = model(input_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_loss_GCNNf.append(criterion(outputs, targets))\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_accuracy_GCNNf.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch [{}/{}], Training Loss: {:.5f}, Test Loss: {:.5f}, Test Accuracy: {:.5f}'.format(\n",
    "                epoch+1, hp['num_epochs'], loss.item(), criterion(outputs, targets), test_accuracy))\n",
    "    \n",
    "    if epoch > 10 and test_accuracy >= best_epoch:\n",
    "        best_epoch = test_accuracy\n",
    "        torch.save({'epoch': epoch+1, \n",
    "                    'accuracy': test_accuracy,\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "               folder_GCNNf+\"model_GCNNf_best.mod\")\n",
    "    \n",
    "print('End training of GCNNf at ' + str(datetime.now()))\n",
    "\n",
    "best_epoch_printer = torch.load(folder_GCNNf+\"model_GCNNf_best.mod\")\n",
    "\n",
    "print('Best epoch of GCNNf: Epoch {} with {}'.format(best_epoch_printer['epoch'], best_epoch_printer['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n",
    "           folder_GCNNf+\"model_GCNNf.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(folder_GCNNf+\"model_GCNNf_best.mod\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "targets_list, targets_shape, predictions_GCNNf, shape_predictions_GCNNf, embeddings = model_test_predict(model, test_data_loader_graph_5f, shape_dict_reverse, graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_viz(embeddings, imagefolder+'GCNNf.png', n_components=2, perplexity=30.0, learning_rate=200, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_data_GCNNf = {'index': [i for i in range(len(predictions_GCNNf))],\n",
    "        'target': targets_list,\n",
    "        'target_shape': targets_shape,\n",
    "        'clas_prediction': predictions_GCNNf,\n",
    "        'shape_prediction': shape_predictions_GCNNf}\n",
    "predictionsDF_GCNNf = pd.DataFrame(pred_data_GCNNf)\n",
    "predictionsDF_GCNNf.to_csv(folder_GCNNf+'predictions_GCNNf.csv')\n",
    "\n",
    "precision_GCNNf, recall_GCNNf, f1_score_GCNNf, support_GCNNf = report_statistics(targets_list, predictions_GCNNf, test_selection, shape_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clas_accuracy_GCNNf = []\n",
    "for shapes in test_selection:\n",
    "    subset = predictionsDF_GCNNf[predictionsDF_GCNNf['target'] == shapes]\n",
    "    clas_accuracy_GCNNf.append(subset['clas_prediction'].value_counts()[shapes]/len(subset))\n",
    "    \n",
    "matrix(predictionsDF_GCNNf, test_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x for x in range(1, len(training_loss_tVeerCNNc)+1)]\n",
    "\n",
    "# creating subplot and figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, training_loss_tVeerCNNc, label = 'tVeerCNNc')\n",
    "ax.plot(x, training_loss_tVeerCNNs, label = 'tVeerCNNs')\n",
    "ax.plot(x, training_loss_tVeerCNNf, label = 'tVeerCNNf')\n",
    "ax.plot(x, training_loss_dCNNc, label = 'dCNNc')\n",
    "ax.plot(x, training_loss_dCNNs, label = 'dCNNs')\n",
    "ax.plot(x, training_loss_dCNNf, label = 'dCNNf')\n",
    "ax.plot(x, training_loss_triangleCNN, label = 'TriangleCNN')\n",
    "ax.plot(x, training_loss_tVeerRNNc, label = 'tVeerRNNc')\n",
    "ax.plot(x, training_loss_tVeerRNNs, label = 'tVeerRNNs')\n",
    "ax.plot(x, training_loss_tVeerRNNf, label = 'tVeerRNNf')\n",
    "ax.plot(x, training_loss_GCNNc, label = 'GCNNc')\n",
    "ax.plot(x, training_loss_GCNNs, label = 'GCNNs')\n",
    "ax.plot(x, training_loss_GCNNf, label = 'GCNNf')\n",
    "ax.plot(x, training_loss_YanGCNNf, label = 'YanGCNNf')\n",
    "\n",
    "# setting labels\n",
    "plt.legend(bbox_to_anchor=(1,0), loc=\"lower left\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training_Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x for x in range(1, len(test_loss_tVeerCNNc)+1)]\n",
    "\n",
    "# creating subplot and figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, test_loss_tVeerCNNc, label = 'tVeerCNNc')\n",
    "ax.plot(x, test_loss_tVeerCNNs, label = 'tVeerCNNs')\n",
    "ax.plot(x, test_loss_tVeerCNNf, label = 'tVeerCNNf')\n",
    "ax.plot(x, test_loss_dCNNc, label = 'dCNNc')\n",
    "ax.plot(x, test_loss_dCNNs, label = 'dCNNs')\n",
    "ax.plot(x, test_loss_dCNNf, label = 'dCNNf')\n",
    "ax.plot(x, test_loss_triangleCNN, label = 'TriangleCNN')\n",
    "ax.plot(x, test_loss_tVeerRNNc, label = 'tVeerRNNc')\n",
    "ax.plot(x, test_loss_tVeerRNNs, label = 'tVeerRNNs')\n",
    "ax.plot(x, test_loss_tVeerRNNf, label = 'tVeerRNNf')\n",
    "ax.plot(x, test_loss_GCNNc, label = 'GCNNc')\n",
    "ax.plot(x, test_loss_GCNNs, label = 'GCNNs')\n",
    "ax.plot(x, test_loss_GCNNf, label = 'GCNNf')\n",
    "ax.plot(x, test_loss_YanGCNNf, label = 'YanGCNNf')\n",
    "\n",
    "# setting labels\n",
    "plt.legend(bbox_to_anchor=(1,0), loc=\"lower left\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Test_Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x for x in range(1, len(test_accuracy_tVeerRNNf)+1)]\n",
    "\n",
    "# creating subplot and figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, test_accuracy_tVeerCNNc, label = 'tVeerCNN+c', c = '#ff7f0e',  ls = ':')\n",
    "ax.plot(x, test_accuracy_tVeerCNNs, label = 'tVeerCNN+s', c = '#ff7f0e',  ls = '--')\n",
    "ax.plot(x, test_accuracy_tVeerCNNf, label = 'tVeerCNN+f', c = '#ff7f0e',  ls = '-')\n",
    "ax.plot(x, test_accuracy_dCNNc, label = 'dCNN+c', c = 'green',  ls = ':')\n",
    "ax.plot(x, test_accuracy_dCNNs, label = 'dCNN+s', c = 'green',  ls = '--')\n",
    "ax.plot(x, test_accuracy_dCNNf, label = 'dCNN+f', c = 'green',  ls = '-')\n",
    "ax.plot(x, test_accuracy_tVeerRNNc, label = 'tVeerRNN+c', c = '#1f77b4',  ls = ':')\n",
    "ax.plot(x, test_accuracy_tVeerRNNs, label = 'tVeerRNN+s', c = '#1f77b4',  ls = '--')\n",
    "ax.plot(x, test_accuracy_tVeerRNNf, label = 'tVeerRNN+f', c = '#1f77b4',  ls = '-')\n",
    "ax.plot(x, test_accuracy_GCNNc, label = 'GCNN+c', c = 'red',  ls = ':')\n",
    "ax.plot(x, test_accuracy_GCNNs, label = 'GCNN+s', c = 'red',  ls = '--')\n",
    "ax.plot(x, test_accuracy_GCNNf, label = 'GCNN+f', c = 'red',  ls = '-')\n",
    "ax.plot(x, test_accuracy_triangleCNN, label = 'LiuDPCN', c = '#7f7f7f', ls = ':')\n",
    "ax.plot(x, test_accuracy_YanGCNNf, label = 'YanGCNN', c = '#7f7f7f')\n",
    "\n",
    "\n",
    "# setting labels\n",
    "plt.legend(bbox_to_anchor=(1,0), loc=\"lower left\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(imagefolder+'model_performances_new.png', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(imagefolder+'model_performances_new.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(folder+ 'results.txt', 'w')\n",
    "text_file.write('net;precision;recall;f1_score;support;' + '\\n')\n",
    "text_file.write('tVeerCNNc;'+str(precision_tVeerCNNc)+';'+str(recall_tVeerCNNc)+';'+str(f1_score_tVeerCNNc)+';'+str(support_tVeerCNNc)+ '\\n')\n",
    "text_file.write('tVeerCNNs;'+str(precision_tVeerCNNs)+';'+str(recall_tVeerCNNs)+';'+str(f1_score_tVeerCNNs)+';'+str(support_tVeerCNNs)+ '\\n')\n",
    "text_file.write('tVeerCNNf;'+str(precision_tVeerCNNf)+';'+str(recall_tVeerCNNf)+';'+str(f1_score_tVeerCNNf)+';'+str(support_tVeerCNNf)+ '\\n')\n",
    "text_file.write('dCNNc;'+str(precision_dCNNc)+';'+str(recall_dCNNc)+';'+str(f1_score_dCNNc)+';'+str(support_dCNNc)+ '\\n')\n",
    "text_file.write('dCNNs;'+str(precision_dCNNs)+';'+str(recall_dCNNs)+';'+str(f1_score_dCNNs)+';'+str(support_dCNNs)+ '\\n')\n",
    "text_file.write('dCNNc;'+str(precision_dCNNf)+';'+str(recall_dCNNf)+';'+str(f1_score_dCNNf)+';'+str(support_dCNNf)+ '\\n')\n",
    "text_file.write('triangleCNN;'+str(precision_triangleCNN)+';'+str(recall_triangleCNN)+';'+str(f1_score_triangleCNN)+';'+str(support_triangleCNN)+ '\\n')\n",
    "text_file.write('tVeerRNNc;'+str(precision_tVeerRNNc)+';'+str(recall_tVeerRNNc)+';'+str(f1_score_tVeerRNNc)+';'+str(support_tVeerRNNc)+ '\\n')\n",
    "text_file.write('tVeerRNNs;'+str(precision_tVeerRNNs)+';'+str(recall_tVeerRNNs)+';'+str(f1_score_tVeerRNNs)+';'+str(support_tVeerRNNs)+ '\\n')\n",
    "text_file.write('tVeerRNNf;'+str(precision_tVeerRNNf)+';'+str(recall_tVeerRNNf)+';'+str(f1_score_tVeerRNNf)+';'+str(support_tVeerRNNf)+ '\\n')\n",
    "text_file.write('GCNNc;'+str(precision_GCNNc)+';'+str(recall_GCNNc)+';'+str(f1_score_GCNNc)+';'+str(support_GCNNc)+ '\\n')\n",
    "text_file.write('GCNNs;'+str(precision_GCNNs)+';'+str(recall_GCNNs)+';'+str(f1_score_GCNNs)+';'+str(support_GCNNs)+ '\\n')\n",
    "text_file.write('YanGCNNf;'+str(precision_YanGCNNf)+';'+str(recall_YanGCNNf)+';'+str(f1_score_YanGCNNf)+';'+str(support_YanGCNNf)+ '\\n')\n",
    "text_file.write('GCNNf;'+str(precision_GCNNf)+';'+str(recall_GCNNf)+';'+str(f1_score_GCNNf)+';'+str(support_GCNNf)+ '\\n')\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
